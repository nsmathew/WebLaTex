\chapter{Literature Survey}

\section{The act of complaining}

As per \cite{olshtain_speechact_1987}, the speech act of complaining in the traditional sense can be understood from the perspective of the speaker stating their displeasure or dissatisfaction to a target entity or individual. This is done as a reaction to an unfavourable event that is currently taking place or has already occurred. The authors believe a few preconditions have to be satisfied to result in a complaint being made. This includes the speaker's belief the entity or individual is responsible for the unfavourable outcome and that the speaker in question suffers from the consequences. The result is a verbally expressed complaint.
\newline \newline
This expression of complaint could be carried out in various ways. The speaker might choose to directly communicate their complaints or concerns to the individual or entity, either immediately after the incident or at a later time. Or they might voice their grievances to others through word-of-mouth or they could even opt to escalate the issue by involving a third party, such as a consumer advocacy office \cite{sparksComplainingCyberspaceMotives2010}.
\newline\newline
The authors of \cite{olshtain_speechact_1987} further delve into the intentions of the speaker in making the complaint. They argue this is carried out with either the hope of repair of the situation or as a 'Face Threatening Act' \cite{brownPolitenessUniversalsLanguage1987}, with the purpose being to damage the face of the individual or entity against whom the complaint is made. In this scenario, a face-threatening act refers to an action that challenges the reputation of the recipient by going against what the recipient desires. These acts can manifest in a verbal form including with variation in tone or inflection or using non-verbal methods.
\newline \newline
While such complaints could be considered direct complaints as per \cite{boxerSocialDistanceSpeech1993}, the authors additionally highlight the use of indirect complaining in speech. In the case of indirect complaints, the speaker does not attribute responsibility for the cause of the complaint to the individual or entity being addressed. The authors theorise, an indirect complaint is used to bring about 'solidarity' between speakers, which is contrary to the use of direct complaints. It can serve as a means to initiate conversations and establish temporary connections with others. The scope of the data for this project (described in the subsequent chapter) is primarily focused on direct complaints as they are selected based on Tweets being addressed to a brand's customer service handle. However it is possible, Tweets which fall into the category of indirect complaints are also included in the dataset.
\newline \newline
Analysing deeper into which types of customers complain more, \cite{sharma_complainers_2010} have looked at how personality traits like impulsivity and self-monitoring impact customer complaining behaviour. \textit{Impulsivity}, as defined by \cite{rookNormativeInfluencesImpulsive1995}, refers to a consistent inclination of customers to act spontaneously and immediately,  without much reflection or careful consideration of available options or potential consequences. This trait remains relatively stable over time for such customers. \textit{Self-monitoring} is described by  \cite{bechererSelfMonitoringModeratingVariable1978} as the propensity to adjust one's behaviour based on the actions or behaviour of others. High self-monitoring individuals are sensitive to others' expressions and behaviour, relying on social cues for their actions, while low self-monitoring individuals may be influenced by personal traits. From their experiments, \cite{sharma_complainers_2010} concluded that individuals with high impulsiveness tend to complain more than those with low impulsiveness, whereas individuals with high self-monitoring tend to complain less than those with low self-monitoring. These effects tend to be more pronounced in situations where the level of dissatisfaction is high. The level of \textit{involvement} a customer has in product or service also matters since the more deeply a customer engages with a consumption scenario, their inclination to invest resources like time, effort, and money into addressing or complaining about an unsatisfactory encounter increases \cite{lauIndividualSituationalFactors2001}. The results from \cite{sharma_complainers_2010} also validated the positive influence a consumer's degree of involvement has on their likelihood of engaging in complaining behaviour.


\section{Complaining online}
The act of complaining exists online in various forms and with varying degrees of intensity and this prevalence lead to the emergence of third-party organisations that provide online channels for customers' ease and convenience \cite{tripp_when_2011}. Notably, there are complaint websites like complaintsboard.com, review websites like trustpilot.com as well as consumer organisations' sites such as consumeraffairs.com, where customers can share their negative experiences and exchange information with others. The impact of negative word of mouth is quite high due to the ease with which negative reports can rapidly reach millions of people, potentially causing significant harm to a company's brand. Various user-generated content platforms such as YouTube, Twitter, and Facebook serve as spaces for expressing complaints. Brands use these platforms for user engagement and this provides the users with the required visibility to potentially raise or escalate an issue. With numerous such options available online, companies can experience significant repercussions arising from actions taken by dissatisfied customers \cite{tripp_when_2011}.
\newline \newline
Of the 431 online complaints assessed by \cite{tripp_when_2011}, 96\% followed what they call a double deviation. This occurs when customers experience both a product or service failure followed by multiple unsuccessful attempts to resolve the issue, resulting in them feeling they have been violated twice. Such customers then resort to online complaining. Their urge to complain online is driven by how they felt betrayed rather than simply being dissatisfied or with any form of malicious intentions to hinder business operations.
\newline\newline
Complaining online is also associated with electronic word-of-mouth or EWOM, which involves sharing information online with a wider group, and it remains accessible over an extended period while often being anonymous \cite{hennig-thurauElectronicWordofmouthConsumeropinion2004}. This type of communication can take place on various platforms, ranging from official company-sponsored sites to unaffiliated blogs. The Internet offers consumers a convenient and anonymous platform to express negative word-of-mouth by sharing their viewpoints and complaints with others. Among the different forms of EWOM, consumer reviews are particularly noteworthy, as they provide valuable insights about products, whether positive or negative \cite{sparksComplainingCyberspaceMotives2010}. Such Negative electronic WOM (EWOM), can significantly damage a brand's reputation and influence potential customers to seek alternative products or services.
\newline \newline
Technology provides an accessible channel that allows consumers to complain with significant ease, making it available to anyone with internet access, even those who may be hesitant to complain directly to the company \cite{sparksComplainingCyberspaceMotives2010}. The reviews and comments posted by consumers online can hold considerable influence over decisions made by other fellow consumers. From an organisation's perspective, the use of online complaining by consumers has some indirect negative consequences as well. The potential experience and knowledge frontline personnel could gain from addressing the complaints directly are lost and this has long-term implications for the organisation. The study by \cite{sparksComplainingCyberspaceMotives2010} on online complaining in the hospitality industry, corroborates the double deviation theory touched upon earlier. Most complaints reviewed involved the individual complaining online after having failed to receive a satisfactory resolution from the hotel staff. Another key finding was the altruistic nature of the complaints, intending to warn other potential customers of the problems. The nature of complaints points to a sense of unfairness being experienced by the guests due to their initial complaints being inadequately addressed and in some cases, combined with a lack of empathy from the staff.

\section{Complaining in social media}
With the increased penetration of social media in the lives of consumers, they now have the ability to express their grievances directly and effectively to service providers using multiple social media platforms \cite{balaji_customer_2015}. Prior to this, a significant portion of dissatisfied customers refrained from lodging complaints due to the perception that the costs associated with complaining far outweighed the benefits linked to resolution \cite{sharma_complainers_2010}. This has had a major impact at the fundamental level on how customers complain across industries. Aside from lodging a complaint, consumers also use the social media channels to publicly vent their anger, a behaviour they formerly exhibited by privately expressing dissatisfaction to friends and family.\\

The study carried out by \cite{balaji_customer_2015} investigated the elements that incite customers to participate in complaining activities via social media. They asserted that the act of complaining through social media was shaped by multiple factors. These include, perceptions of not being treated fairly and wanting retribution, attributions of causality and the personal identity and traits of the individual. Particularly, they found that distinct factors impact both private and public complaining communicated through social media. They found the level of perceived unfairness in the situation had more impact on customers choosing to publicly complain through social media. Additionally, a positive view of the firm's compensation terms fostered the expectation that complaining increased the chance of resolution. Consequently, dissatisfied customers were more inclined to express their grievances privately to the service provider instead of publicly criticising them on social media.\\

Organisations have also been promoting the use of social media and digital channels as a medium for providing customer support while moving away from conventional contact points like call centres \cite{sunDoesActiveService2021}. The cost of handling per customer on Twitter, for example, is significantly lower when compared with a call centre. It makes the situation convenient for the customer as well since they can interact with a brand via the social media handles at their preferred time rather than having to wait for a prolonged time over a phone call. Effectively managing a customer's complaint also has the potential to transform a service failure into a favourable brand encounter for the individual customer as well as showcase the brand in a positive light among other social media users.\\

Utilising data from Twitter with the brand accounts of international airlines as the basis, the study \cite{sunDoesActiveService2021} set out to determine if active service intervention of complaints online was driving further complaining on social media. They found, while increased service activity did lead to a rise in customer complaints on social media, improved customer service quality reduced future complaints. They felt if a substantial number of discontented customers, who might not otherwise use conventional contact methods, opt for social media support, it was likely to lead to reduced customer attrition for the companies over time. However, they suggested caution, since this multiplier effect could be harmful if the failure of services gained traction over the actual success cases.\\

Widespread negative portrayals of brands over social media are a current reality. The study conducted by \cite{ahluwaliaConsumerResponseNegative2000} aimed to comprehend how consumers handle negative publicity concerning the brands they favour and use. The researchers discovered that committed brand supporters counter negative information about the brand to a much higher degree when compared to consumers who are less committed to that brand. Strikingly, although low-commitment consumers exhibit similar brand affinity to high-commitment consumers, they show greater shifts in attitudes and increased indecision when exposed to negative brand information. Additionally, the study found that consumers prioritise negative information due to its higher diagnostic value. This inclination toward negativity becomes more pronounced when consumer commitment to the subject in question is low. Conversely, at higher commitment levels, not only does the negativity effect largely go away, but positive information about the subject is perceived as more diagnostically meaningful than negative information.\\

The speed and scale of communication on social media is a key factor for opinion propagation online \cite{pfefferUnderstandingOnlineFirestorms2014}. Posts on social media provide a continuous stream of information, where each new piece of information supersedes the previous one. When it comes to information that appeals to a wider audience, a considerable number of individuals can be reached swiftly. Consequently, this may lead to a situation where a particular topic gains prominence for a short while, triggering a surge in posts on that topic. Unlike traditional newspapers, which operate on a daily communication cycle, the realm of social media demands swift responses from affected organisations, often within hours or minutes.\\

Drawing from complaints on Twitter and Facebook, the survey conducted by \cite{istanbulluogluComplaintHandlingSocial2017} reveals a strong link between customer satisfaction and response time. The study observed that, although anticipated response times were relatively brief for these social media platforms, distinct expectations exist based on the specific services. Participants expected Twitter responses within the range of 1 to 3 hours, while on Facebook, the expected timeframe increased to 3 to 6 hours. Surprisingly, the actual response times were quite comparable for both platforms, averaging around 1 to 3 hours. Delving deeper into how companies respond, it's argued that resolving an issue with a single initial response might not be feasible. Companies, being cognizant of this, employ initial responses to gather additional problem details and, when applicable, direct users to relevant departments. The findings emphasize that a swifter initial response, coupled with a more expedited overall issue resolution, leads to higher satisfaction with the complaint-handling process.\\

\section{Twitter as a medium for self-expression}
Twitter is a microblogging platform with over 540 million users and active users amounting to over 250 million\footnote{\url{https://www.reuters.com/business/media-telecom/musk-says-x-monthly-users-reach-new-high-2023-07-28/}}. It is known for being the fastest social media platform due to its high turnover of information and short message length \cite{istanbulluogluComplaintHandlingSocial2017}. This short and quick communication is influenced by both technical factors and the nature of interactions, where opinions are often simplified to likes or upvotes. Additionally, messages are usually limited in length enforced by Twitter's character limit (currently at 280 for most users\footnote{\url{https://developer.twitter.com/en/docs/counting-characters}}), to accommodate these constraints. \\

Different communication modalities exist for various social media platforms. Twitter uses short messages, primarily in text but could include images and videos while Instagram has images at its core, and Facebook combines both approaches along with privacy controls \cite{shane-simpsonWhyCollegeStudents2018}. The study performed by \cite{shane-simpsonWhyCollegeStudents2018} found that these varying aspects attract different types of consumers to these platforms. It also found that participants who were primarily Twitter users openly shared content leading to increased social capital. Online social capital while often quantifiable from the number of followers or connections and likes or views accumulates from digital interactions, which may or may not align with a user's perceived value in the online \cite{faucherSocialCapitalOnline2018}. Users utilise such indicators to achieve specific objectives as part of their online social life. Participants favouring Twitter appeared to exhibit stronger individualistic tendencies and gave importance to self-expression and choice. They also showed a greater inclination towards forming and dissolving relationships, and trust in unfamiliar individuals. This group tended to be younger and showed heightened engagement in self-expression on public online platforms. They also found such users often set their profiles for public visibility while building up social connections \cite{shane-simpsonWhyCollegeStudents2018}.\\

The authors of \cite{qiuYouAreWhat2012} explored how personality affects the content and tone of Tweets. Microblogging on Twitter generates an extensive written record of daily behaviour in natural settings due to the active usage of the platform by millions of users. They found Tweets to be containing valid linguistic cues to the user's personality. They suggest the linguistic characteristics of the Tweets in terms of the length of the messages, variation in functional vocabulary and type of emotion used depends on the personality traits of the individuals including the level of extraversion, agreeableness and conscientiousness.

\section{NLP based solutions}
Computational linguistics or Natural Language Processing (NLP) has evolved into a compelling scientific field with significant research being carried out while having practical usage across both consumer use cases as well business operations. NLP focuses on using computational methods to comprehend, generate, and learn from human language content \cite{hirschbergAdvancesNaturalLanguage2015}. The progress in this field over the last decade and more is attributed to four main factors by \cite{hirschbergAdvancesNaturalLanguage2015}: significant computing power advancements, access to extensive linguistic data, the success of advanced machine learning techniques, and a more comprehensive understanding of human language's structure with the construct of social usage. The solutions target to solve problems in areas of machine translation, text generation, summarisation and classification, question answering as well generating business insight from the vast amounts of text available online.\\

While the approaches used during the nascent stages of NLP development involved rule-based methods there were a number of challenges including the unique nature of human languages being context-dependent and the inherent ambiguity involved. As larger amounts of text from multiple domains started becoming available digitally, statistical approaches using text corpora and 'bag-of-words' started gaining prominence. However, significant progress was made with machine learning methods evolving in conjunction with researchers gathering a more intricate understanding of linguistics via computational methods in terms of syntax, semantics and context \cite{hirschbergAdvancesNaturalLanguage2015}.\\

\subsection{Recurrent Neural Networks}
Recurrent Neural Networks (RNN) are an extension of the traditional feedforward neural network and were designed to process input sequences of varying lengths. RNNs include hidden states (recurring) that persist through time. RNNs have difficulty in capturing long-range dependencies due to the problem of exploding or vanishing gradients \cite{chungEmpiricalEvaluationGated2014}. RNNs are also deficient in effectively propagating and correcting errors as part of backpropagation through time, leading to a slow learning process as well as difficulty in capturing long-term dependencies \cite{chungEmpiricalEvaluationGated2014}. Long Short-Term Memory or LSTM networks were proposed by \cite{chungEmpiricalEvaluationGated2014} to solve these challenges by using a gradient-based algorithm that ensured a constant flow of the error through the internal states of special units without being affected by either the vanishing or exploding gradient problem. These special units were termed memory cells and gate units. \cite{hochreiterLongShortTermMemory1997}. Along the lines of LSTM, Gated Recurrent Units or GRUs were introduced by \cite{choPropertiesNeuralMachine2014} to solve the issue of gradient instability and to better handle long-range dependencies. They use gate units, which decide the amount of past information to be carried forward along with how much new information is to be included at each time step. This type of gating mechanism helps GRUs to adjust their memory by deciding what to update and forget, making them more efficient at modelling long sequences.\\

These networks used what is known as encoder-decoder architectures. The encoder processes a variable-length input (source sentence) and builds a ﬁxed-length vector representation. The decoder then generates the target sentence as a variable-length output by conditioning on the output from the encoder \cite{choPropertiesNeuralMachine2014}. Using translation as an example, the task is to learn the conditional distribution $p(target | source)$ for a target sentence and an input sentence. Now the target sentence can be predicted from a source sentence using this conditional distribution by maximising the probability \cite{choPropertiesNeuralMachine2014}.\\

An inherent concern with the encoder-decoder method is how effectively it can condense all the important details of a source sentence into a fixed-size vector in a machine translation task for example. This poses difficulties for the network when dealing with lengthy sentences. \cite{bahdanauNeuralMachineTranslation2016} proposed an enhancement to this model to alleviate this issue by introducing the attention mechanism. In the updated version, the model determines the key information from positions in the source sentence for every word that is being translated as the 'attention'. The prediction for the target word by the decoder uses this additional context combined with all the target words that have already been predicted. With this approach, rather than attempting to fit in all the information by encoding a source sentence into a fixed-size vector, a sequence of vectors is generated and the decoder determines what information to use. The authors were able to show this enhanced approach was able to handle longer sentences without significant deterioration of translation performance \cite{bahdanauNeuralMachineTranslation2016}.\\

\subsection{Transformer network and inductive transfer learning}
While the attention mechanism assisted in dealing with larger sentences they did so at the expense of increased computation cost as well as having limited capability for parallelisation \cite{vaswaniAttentionAllYou2023a}. The authors of \cite{vaswaniAttentionAllYou2023a} used the self-attention mechanism to build an auto-regressive Transformer network made up of repeated encoder and decoder stacks sans recurrence. Self-attention allows the network to identify and arrive at a representation of the sequence of text by looking at the entire input sequence at each step. This mechanism aids the model in developing a better understanding of the input data and the relationships between the various parts of the input.\\

\cite{vaswaniAttentionAllYou2023a} describe the encoder made up of a stack of identical layers and generate for an input sequence $[x_1, ..., x_n]$ a representation sequence $[z_1, ..., z_n]$. This representation sequence is utilised by the decoder which is also stacked to generate an output sequence as $[y_1, ..., y_m]$. At each stage, the decoder uses the previously generated output sequence to create the next token in the sequence making the model auto-regressive. The authors also introduced the concept of multi-head attention where instead of performing attention just once at each step it is performed multiple times (8 were used in the paper). However, the dimensionality is reduced such that when the final output of all the attention projections is concatenated they have the same dimensionality as single-head attention. This limits any impact on computation cost. The results from their experiments showed this architecture had a performance advantage over previous BLEU\footnote{Bilingual Evaluation Understudy, an evaluation method for machine translation of human language.} benchmark scores for English-to-German and English-to-French translation tasks. They also showed significant savings in terms of training computation cost due to the level of parallelisation that was achieved \cite{vaswaniAttentionAllYou2023a}. \\

Inductive transfer is a transfer learning technique in machine learning to leverage the training process and apply learnings and representations from one task onto a different task that has a different data distribution \cite{howardUniversalLanguageModel2018}. Considering the limited success of this technique in NLP tasks, \cite{howardUniversalLanguageModel2018} identified key challenges, reflecting on past research.  While computer vision tasks had found success in implementing inductive transfer learning, the authors felt some of the issues faced by NLP tasks were induced by researchers not appreciating the differences between the two task categories. Recognizing potential benefits in tailoring approaches, they proposed a method that pre-trained a language model on Wikitext-103 \cite{merityPointerSentinelMixture2016} representing the general domain dataset. This was followed by fine-tuning the language model using data from the various domains scoped for their experiments. They concluded the network with 2 layers for task-specific classification which were fine-tuned using data from the target task's domain. Their approach was able to achieve state-of-the-art results on tasks for sentiment analysis, question answering and topic classification.\\

\subsection{BERT and its variants}
Bidirectional Encoder Representations from Transformers or BERT was proposed by \cite{devlinBERTPretrainingDeep2018} to solve some of the issues with the Transformer models due to their use of unidirectional language models during the pre-training phase which limits the strengths of pre-trained representations. They proposed using two pre-training tasks. The first was a masked language model or MLM where a token has been randomly masked and the objective is to predict based on the context. They argued this would assist in pre-training deeper and bi-directional Transformer models. The second task involved pre-training sequence pairs and was termed as next sentence prediction or NSP. They showed that using very large model sizes could also result in notable enhancements for even smaller downstream tasks, as long as the model was adequately pre-trained. Their experiments were run with 2 versions of BERT, BERT Base with 110M parameters and BERT LARGE with 340M parameters. The BERT models demonstrated superior performance compared to other state-of-the-art models across various benchmarks encompassing tasks related to natural language comprehension, inference, and question-answering.\\

With the significant success of the BERT Transformer model, several derived models were proposed over the next few years with some of them focusing on improving the pre-training while others were on reducing the model size. Some of the important ones are covered in a brief. RoBERTa or Robustly Optimised BERT Pretraining Approach was introduced by \cite{liuRoBERTaRobustlyOptimized2019} to overcome what the authors felt were shortcomings in BERT's \cite{devlinBERTPretrainingDeep2018} pretraining strategy including certain design choices. They carried out changes in terms of learning rate warmup and tuning the Adam optimiser. They also increase the pretraining with additional corpus. With these changes, the model achieves state-of-the-art benchmark results.\\

While larger models with appropriate pretraining have better predictive performance \cite{devlinBERTPretrainingDeep2018, liuRoBERTaRobustlyOptimized2019} there exist challenges posed by memory limitations and training times associated with model size increases. A Lite BERT or ALBERT was introduced by \cite{lanALBERTLiteBERT2020} to include techniques to enhance the efficiency of large-scale pre-trained language models. ALBERT employed two methods: factorised embedding parameterisation and cross-layer parameter sharing. The former divided the embedding matrix into smaller components, allowing hidden layers to expand without a substantial rise in vocabulary embedding parameters. The latter controlled parameter growth as network depth increased. These approaches significantly reduced parameters while maintaining performance. ALBERT with BERT Large like configuration boasts 18 times fewer parameters and 1.7 times faster training time. Their experiments showed ALBERT xxlarge, with about 70\% of BERT Large's parameters had significant improvements compared to BERT-large across benchmark downstream tasks with speedup achieved in training times as well.\\

DistilBERT \cite{sanhDistilBERTDistilledVersion2020}, a variation of BERT with the knowledge distillation technique applied in the pretraining phase was another model proposed to address similar challenges. Knowledge distillation involves distilling the learnings of a larger 'teacher' model (or ensemble) to a smaller and compact 'student' model as part of training and hence is a form of a compression technique \cite{sanhDistilBERTDistilledVersion2020}. The results from the experiments showed this model had 40\% fewer parameters than BERT Base but was able to achieve performance on downstream benchmark tasks within just 0.6\% to 3.9\% points behind BERT Base. Another goal of the authors was reduction in inference time, which they achieved by the model being faster by up to 60\%.\\

MobileBERT \cite{sunMobileBERTCompactTaskAgnostic2020} was proposed around the same period as DistilBERT and uses knowledge transfer techniques from a larger teacher model to a student model. The researchers stated their motivation was to create a smaller version of BERT that could be deployed on mobile devices which are generally resource-constrained. The model has smaller layers but retains the same depth as BERT Large by incorporating  bottleneck structures and stacked feed-forward networks. While MobileBERT was 4.3 times smaller than BERT Base, the results from the benchmark tests showed it was only 0.6\% points lower in performance but faster by 5.5 times on a mobile device.\\

To develop models that align to pre-defined memory and computational speed requirements, \cite{turcWellReadStudentsLearn2019} proposed various versions of BERT with changes in the model size and size of the embeddings. They used various strategies to train a student compact model, combined with a bigger teacher model. The smallest model was Transformer Tiny (BERT Tiny) with  4.4M parameters and an embedding size of 128 while the largest was BERT Base with 110M parameters. They used various techniques to develop the compact model and compared them with BERT Large as the baseline on 3 NLP tasks. Pretraining the compact model with a masked language model objective and following it with knowledge distillation provided the best performance. This approach was superior to using distillation alone or relying on pretraining combined with fine-tuning.\\

To conclude this section, an approach of pre-training BERT on English Tweets to enhance performance on Twitter data is exmined. BERTweet Base was introduced by \cite{nguyenBERTweetPretrainedLanguage2020} as the first pre-trained model on large-scale Tweets dataset in the public domain. While retaining the BERT Base architecture they used the pre-training optimisations introduced by RoBERTa [cite ROBERTA]. They argued the linguistic attributes of Tweets like length, grammar and vocabulary used differ from the text present in the corpora used for pre-training BERT. The pretraining was performed using a corpus of 850M tweets. This was followed by fine-tuning the model independently for each of the 3 downstream Tweet NLP tasks. Their results showed the model did better than RoBERTa Base in all 3 NLP tasks. While RoBERTa Large did outperform BERTweet Base on the POS tagging task, they speculate this could be attributed to the significant difference in model sizes.

\subsection{Ongoing research}
GPT or Generative Pre-trained Transformer models have significantly changed the landscape in terms of the general perception of AI as well its applicability. As per OpenAI, the developers of ChatGPT and other GPT-[2,3,3.5,4] versions, their GPT models have undergone training to comprehend both natural language and code. These models generate textual responses based on their inputs. The inputs given to GPT models are commonly referred to as 'prompts'. GPTs enable the development of applications for tasks such as writing documents, coding, answering questions, text analysis, creating dialogue agents and language translation among others\footnote{\url{https://platform.openai.com/docs/guides/gpt}}. The use of prompts allows for performing text classification tasks as well. Some of the recent research on such models will be discussed next.\\

When using Transformers, one of primary drawbacks is that despite the task-agnostic nature, datasets relevant to that task along with fine-tuning are still essential for good performance \cite{brownLanguageModelsAre2020}. This dependency on extensive labelled datasets limits the wider application of language models. Additionally, drawing a parallel with humans, they typically require minimally supervised datasets to grasp most language tasks. The authors contend the aspiration is for NLP systems to eventually achieve similar versatility and broadness. Meta-learning, where a language model learns a wide skill set during training and applies them at inference to adapt quickly to tasks is an approach researched in recent years. Despite some initial promise, this approach lags fine-tuning in terms of results. Another promising direction involves increasing the size of the Transformer language models, leading to enhanced NLP performance \cite{brownLanguageModelsAre2020}. The authors argue due to the nature of meta-learning, larger models might exhibit significant improvements in in-context learning abilities. To validate this, the researchers built GPT-3 \cite{brownLanguageModelsAre2020}, an auto-regressive model with 175 billion parameters.\\

GPT-3 was evaluated across three conditions, 'few-shot learning' with multiple demonstrations within the context window, 'one-shot learning' with a single demonstration, and 'zero-shot' learning, relying solely on natural language directions. The model demonstrated good performance across NLP tasks and benchmarks with the 3 evaluation conditions. It closely rivalled state-of-the-art fine-tuned systems in certain cases. For ad-hoc tasks the model exhibited high qualitative results. Finally, the authors assessed the social impact of using such models and highlighted the models does show varying levels of bias across race, religion and gender. They reason this is likely arising from the inherent stereotypes captured within the training data sourced from the internet \cite{brownLanguageModelsAre2020}.\\

Capable of processing both images and text as input, GPT-4 \cite{openaiGPT4TechnicalReport2023} is a large Transformer based multimodal model, that generates text as its output. It is pre-trained to predict the next token as the output. Such models have a broad area of application with machine translation, text summarisation and dialogue systems as some of the main use cases. The authors \cite{openaiGPT4TechnicalReport2023} claim the strength of GPT-4 is in understanding and generating natural language in intricate and complicated situations and show it performs highly in exams designed for humans. While the model is superior in performance to other state-of-the-art systems in both English and other languages it suffers from limitations including 'hallucinations', lack of capability to learn from experience,  restricted context and inherent bias. While GPT-4 alleviates the hallucination problem when compared to GPT-3.5 (successor to GPT-3), authors suggest usage with caution, especially in mission-critical scenarios. GPT-4 can handle prompts containing both images and text, allowing users to define various vision or language tasks. The model generates text outputs based on mixed text and image inputs, displaying comparable proficiency across different domains as it does with text-only inputs \cite{openaiGPT4TechnicalReport2023}. \\

While the model is extremely versatile and outperforms other state-of-the-art models in tests, the authors acknowledge the risks associated with the higher capabilities and are working with other researchers on recommendations on how society can prepare itself for any potential social and economic impacts from the wider usage of such AI technologies \cite{openaiGPT4TechnicalReport2023}.

\section{Previous work on automation}
The first paper on the computational linguistic analysis or complaints and using machine learning approaches to automate identifying complaints in social media was researched by \cite{preotiuc-pietro_automatically_2019}. Aside from creating a publically available and annotated Twitter dataset for complaints, the authors performed a broad assessment of quantitative features of the comaplaints data. They looked at how unigrams, Linguistic Inquiry and Word Count or LIWC and clusters of words perform as features for the classification task. Additionally they attempted to 
