@article{ahluwaliaConsumerResponseNegative2000,
  title      = {Consumer {Response} to {Negative} {Publicity}: {The} {Moderating} {Role} of {Commitment}},
  volume     = {37},
  issn       = {0022-2437},
  shorttitle = {Consumer {Response} to {Negative} {Publicity}},
  url        = {https://doi.org/10.1509/jmkr.37.2.203.18734},
  doi        = {10.1509/jmkr.37.2.203.18734},
  abstract   = {Even though negative information about brands and companies is widely prevalent in the marketplace, except for case studies, there has been no systematic investigation of how consumers process negative information about the brands they like and use. In the three studies in this research, the authors attempt to bridge this gap. The findings of the first and second studies provide a theoretical framework for understanding how consumers process negative information in the marketplace. Commitment of the consumer toward the brand is identified as a moderator of negative information effects. In the third study, the authors use this theoretical framework to derive and test response strategies that companies can use to counter negative publicity for consumers who are high and low in commitment toward the brand.},
  language   = {en},
  number     = {2},
  urldate    = {2023-08-21},
  journal    = {Journal of Marketing Research},
  author     = {Ahluwalia, Rohini and Burnkrant, Robert E. and Unnava, H. Rao},
  month      = may,
  year       = {2000},
  note       = {Publisher: SAGE Publications Inc},
  pages      = {203--214},
  file       = {SAGE PDF Full Text:/Users/nitinmathew/Zotero/storage/LQIKJKAV/Ahluwalia et al. - 2000 - Consumer Response to Negative Publicity The Moder.pdf:application/pdf}
}

@article{artsteinInterCoderAgreementComputational2008,
  title     = {Inter-{Coder} {Agreement} for {Computational} {Linguistics}},
  volume    = {34},
  copyright = {Copyright 2020 Elsevier B.V., All rights reserved.},
  issn      = {0891-2017},
  abstract  = {This article is a survey of methods for measuring agreement among corpus annotators. It exposes the mathematics and underlying assumptions of agreement coefficients, covering Krippendorff's alpha as well as Scott's pi and Cohen's kappa; discusses the use of coefficients in several annotation tasks; and argues that weighted, alpha-like coefficients, traditionally less used than kappa-like measures in computational linguistics, may be more appropriate for many corpus annotation tasks—but that their use makes the interpretation of the value of the coefficient even harder.},
  language  = {eng},
  number    = {4},
  journal   = {Computational linguistics - Association for Computational Linguistics},
  author    = {Artstein, Ron and Poesio, Massimo},
  year      = {2008},
  note      = {Place: One Rogers Street, Cambridge, MA 02142-1209, USA
               Publisher: MIT Press},
  keywords  = {Language \& Linguistics, Linguistics, Social Sciences, annotation, Applied linguistics, Computational linguistics, Computer Science, Computer Science Artificial Intelligence, Computer Science Interdisciplinary Applications, Content analysis, language, linguistic corpus, mathematical linguistics, Methods, Polls \& surveys, Science \& Technology, statistical approach, Studies, Survey Article, Technology},
  pages     = {555--596}
}

@article{balaji_customer_2015,
  title     = {Customer e-complaining behaviours using social media},
  volume    = {35},
  copyright = {2015 Taylor \& Francis 2015},
  issn      = {0264-2069},
  abstract  = {This paper develops a conceptual framework about customer complaining behaviours (CCB), using social media. Specifically, this research expands the current understanding of CCB by examining the differential impact of unfairness, firm response, retaliation, locus attribution, stability attribution, and personal identity on public complaining and private complaining using social media, and their subsequent impact on post-complaining satisfaction (PCS) and loyalty. Public complaining refers to customer complaints directed to a service provider, while private complaining refers to service failure complaints directed towards other customers. A structural equation model shows that high levels of unfairness, firm response, locus, and personal identity have a strong influence on public complaining, while desire for retaliation is a significant factor influencing private complaining. The findings contribute to practice by providing useful and pertinent information for developing suitable web care interventions to effectively deal with public complaining and private complaining through social media platforms.},
  language  = {eng},
  number    = {11-12},
  journal   = {The Service industries journal},
  author    = {Balaji, M. S. and Jha, Subhash and Royne, Marla B.},
  year      = {2015},
  note      = {Place: London
               Publisher: Routledge},
  keywords  = {Analysis, attribution theory, Brand loyalty, complaining behaviour, Complaints, Consumer behavior, Consumer complaints, Customer relationship management, Customer satisfaction, Identity, justice theory, Management, Quantitative psychology, service failure, Social media, Social networks, Studies, Technology application, User behavior},
  pages     = {633--654}
}

@article{bechererSelfMonitoringModeratingVariable1978,
  title    = {Self-{Monitoring} as a {Moderating} {Variable} in {Consumer} {Behavior}},
  volume   = {5},
  issn     = {0093-5301},
  doi      = {10.1086/208726},
  abstract = {Most consumer research uses either a dispositional or a situational conceptual orientation for all types of behavior. This article explores the value of using a moderating variable to identify which individuals are primarily influenced by either dispositional or situational variables.},
  language = {eng},
  number   = {3},
  journal  = {The Journal of consumer research},
  author   = {Becherer, Richard C. and Richard, Lawrence M.},
  year     = {1978},
  note     = {Place: CHICAGO
              Publisher: Journal of Consumer Research},
  keywords = {Brand names, Brand preferences, Business, Business \& Economics, Consumer attitudes, Consumer behavior, Consumer research, Correlation coefficients, House brands, Inventory, Marketing, National brands, Personality, Regression analysis, Regression coefficients, Sample size, Self study, Social Sciences, Socialization, Variable coefficients, Variables},
  pages    = {159--162}
}

@misc{bhargavaGeneralizationNLIWays2021,
  title      = {Generalization in {NLI}: {Ways} ({Not}) {To} {Go} {Beyond} {Simple} {Heuristics}},
  shorttitle = {Generalization in {NLI}},
  url        = {http://arxiv.org/abs/2110.01518},
  doi        = {10.48550/arXiv.2110.01518},
  abstract   = {Much of recent progress in NLU was shown to be due to models' learning dataset-specific heuristics. We conduct a case study of generalization in NLI (from MNLI to the adversarially constructed HANS dataset) in a range of BERT-based architectures (adapters, Siamese Transformers, HEX debiasing), as well as with subsampling the data and increasing the model size. We report 2 successful and 3 unsuccessful strategies, all providing insights into how Transformer-based models learn to generalize.},
  urldate    = {2023-08-04},
  publisher  = {arXiv},
  author     = {Bhargava, Prajjwal and Drozd, Aleksandr and Rogers, Anna},
  month      = oct,
  year       = {2021},
  note       = {arXiv:2110.01518 [cs]},
  keywords   = {Computer Science - Computation and Language},
  annote     = {Comment: Workshop on Insights from Negative Results (EMNLP 2021)},
  file       = {arXiv Fulltext PDF:/Users/nitinmathew/Zotero/storage/HPUQ7UDR/Bhargava et al. - 2021 - Generalization in NLI Ways (Not) To Go Beyond Sim.pdf:application/pdf;arXiv.org Snapshot:/Users/nitinmathew/Zotero/storage/LWKVUAC9/2110.html:text/html}
}

@article{boxerSocialDistanceSpeech1993,
  title     = {Social distance and speech behavior: {The} case of indirect complaints},
  volume    = {19},
  copyright = {1993},
  issn      = {0378-2166},
  abstract  = {This paper discusses speech-act research with specific reference to Wolfson's ‘Bulge’ theory of social distance. The theory maintains that most solidarity-establishing speech behavior takes place among status-equal friends and acquaintances. Wolfson built her theory on evidence from data collected on two speech acts: compliments and invitations. Counterevidence is given here on another rapport-inspiring speech-act sequence termed indirect complaints/commiseration. Indirect complaining/commiserating is described in detail with specific reference to the effect of the variable of social distance on its distribution. The way in which indirect complaints and commiserative responses pattern out along the social distance continuum indicates that the characteristic Bulge is not in the middle (among friends and acquaintances) but is always skewed toward one end of the continuum (strangers) or the other (intimates). In contradistinction to the way in which compliments and invitations appear to be socially distributed with reference to social distance, commiseration occurs almost as frequently among strangers as it does among friends. The conclusion to be drawn is that some rapport-inspiring speech behaviors occur almost as frequently among interlocutors of extreme social distance as they do among friends and acquaintances.},
  language  = {eng},
  number    = {2},
  journal   = {Journal of pragmatics},
  author    = {Boxer, Diana},
  year      = {1993},
  note      = {Place: AMSTERDAM
               Publisher: Elsevier B.V},
  keywords  = {complaining, Conversational Analysis, English language (Modern), Interaction, Interpersonal Communication, Interpersonal Relations, Language \& Linguistics, Linguistics, Meaning, Pragmatics, Social Distance, Social Relations, Social Sciences, social status, Sociolinguistics and ethnolinguistics, Speech, speech acts, The English Language},
  pages     = {103--125}
}

@book{brownPolitenessUniversalsLanguage1987,
  address   = {Cambridge [Cambridgeshire] ; New York},
  series    = {Studies in interactional sociolinguistics ; 4},
  title     = {Politeness : some universals in language usage},
  isbn      = {0-521-30862-3},
  abstract  = {"This study is about the principles for constructing polite speeches. The core of it first appeared in Questions and Politeness, edited by Esther N. Goody (now out of print). It is here reissued with a fresh introduction that surveys the considerable literature in linguistics, psychology and the social sciences that the original extended essay stimulated, and suggests distinct directions for research. The authors describe and account for some remarkable parallelisms in the linguistic construction of utterances with which people express themselves in different languages and cultures. A motive for these parallels is isolated - politeness, broadly defined to include both polite friendliness and polite formality - and a universal model is constructed outlining the abstract principles underlying polite usages. This is based on the detailed study of three unrelated languages and cultures: the Tamil of South India, the Tzeltal spoken by Mayan Indians in Chiapas, Mexico, and the English of the USA and England, supplemented by examples from other cultures. Underneath the apparent diversity of polite behaviour in different societies lie some general pan-human principles of social interaction, and the model of politeness provides a tool for analysing the quality of social relations in any society. This volume will be of special interest to students in linguistic pragmatics, sociolinguistics, applied linguistics, anthropology, and the sociology and social psychology of interaction."–Publisher's web site.},
  language  = {eng},
  publisher = {Cambridge University Press},
  author    = {Brown, Penelope and {Levinson, Stephen C}},
  year      = {1987},
  lccn      = {86023255},
  keywords  = {Linguistics, Pragmatics, Speech, Colloquial language, Conduct of life, Conversation, Etiquette, Exchange theory (Sociology), General semantics, Integrational linguistics (Oxford school), Language and culture, Language and languages – Grammars, Language and languages – Philosophy, Language and languages – Usage, Levinson Stephen C, Logic Symbolic and mathematical, Manners and customs, Oral communication, Psychology, Semantics (Philosophy), Social interaction, Social psychology, Sociolinguistics, Sociology, Speech acts (Linguistics)}
}

@article{chungEmpiricalEvaluationGated2014,
  title    = {Empirical {Evaluation} of {Gated} {Recurrent} {Neural} {Networks} on {Sequence} {Modeling}},
  url      = {https://arxiv.org/abs/1412.3555},
  doi      = {10.48550/arxiv.1412.3555},
  abstract = {In this paper we compare different types of recurrent units in recurrent
              neural networks (RNNs). Especially, we focus on more sophisticated units that
              implement a gating mechanism, such as a long short-term memory (LSTM) unit and
              a recently proposed gated recurrent unit (GRU). We evaluate these recurrent
              units on the tasks of polyphonic music modeling and speech signal modeling. Our
              experiments revealed that these advanced recurrent units are indeed better than
              more traditional recurrent units such as tanh units. Also, we found GRU to be
              comparable to LSTM.},
  language = {eng},
  urldate  = {2023-08-08},
  author   = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  year     = {2014}
}

@article{coussement_improving_2008,
  title     = {Improving customer complaint management by automatic email classification using linguistic style features as predictors},
  volume    = {44},
  copyright = {2007 Elsevier B.V.},
  issn      = {0167-9236},
  abstract  = {Customer complaint management is becoming a critical key success factor in today's business environment. This study introduces a methodology to improve complaint-handling strategies through an automatic email-classification system that distinguishes complaints from non-complaints. As such, complaint handling becomes less time-consuming and more successful. The classification system combines traditional text information with new information about the linguistic style of an email. The empirical results show that adding linguistic style information into a classification model with conventional text-classification variables results in a significant increase in predictive performance. In addition, this study reveals linguistic style differences between complaint emails and others.},
  language  = {eng},
  number    = {4},
  journal   = {DECISION SUPPORT SYSTEMS},
  author    = {Coussement, Kristof and Van den Poel, Dirk},
  year      = {2008},
  note      = {Place: AMSTERDAM
               Publisher: Elsevier B.V},
  keywords  = {Analysis, Applied sciences, Artificial intelligence, Automatic email classification, Call-center email, Classification, Complaints, Computer science, Computer Science, Computer Science Artificial Intelligence, Computer Science Information Systems, control theory, Customer Complaint Handling, Decision theory. Utility theory, Electronic mail systems, Exact sciences and technology, Information, Latent Semantic Indexing (LSI), Linguistics, Operational research and scientific management, Operational research. Management science, Operations Research \& Management Science, Science \& Technology, Singular Value Decomposition (SVD), Speech and sound recognition and synthesis. Linguistics, Studies, systems, Technology, Voice of customers (VOC)},
  pages     = {870--882}
}



@article{devlinBERTPretrainingDeep2018,
  title     = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
  copyright = {http://arxiv.org/licenses/nonexclusive-distrib/1.0},
  abstract  = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  language  = {eng},
  author    = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year      = {2018},
  keywords  = {Computer Science - Computation and Language}
}


@book{faucherSocialCapitalOnline2018,
  address    = {London, London, England},
  series     = {{CDSMS} ({Series})},
  title      = {Social {Capital} {Online}: alienation and accumulation},
  isbn       = {978-1-911534-56-3},
  shorttitle = {Social {Capital} {Online}},
  abstract   = {Introduction : What is online social capital? -- Online social capital as social -- Online social capital as capital -- Capitalism and the ideologies of the social -- From accumulation to alienation : Marx and Veblen -- Alienation 2.0 : symptoms of narcissism and aggression -- The network spectacle -- Conclusion., "What is 'social capital'? The enormous positivity surrounding it conceals the instrumental economic rationality underpinning the notion as corporations silently sell consumer data for profit. Status chasing is just one aspect of a process of transforming qualitative aspects of social interactions into quantifiable metrics for easier processing, prediction, and behavioural shaping. A work of critical media studies, Social Capital Online examines the idea within the new 'network spectacle' of digital capitalism via the ideas of Marx, Veblen, Debord, Baudrillard and Deleuze. Explaining how such phenomena as online narcissism and aggression arise, Faucher offers a new theoretical understanding of how the spectacularisation of online activity perfectly aligns with the value system of neoliberalism and its data worship. Even so, at the centre of all, lie familiar ideas - alienation and accumulation - new conceptions of which he argues are vital for understanding today's digital society."},
  language   = {eng},
  publisher  = {University of Westminster Press},
  author     = {Faucher, Kane X.},
  year       = {2018},
  keywords   = {Information technology, Social aspects; Social capital (Sociology); Social media, Social aspects; Sociology}
}


@article{hennig-thurauElectronicWordofmouthConsumeropinion2004,
  title      = {Electronic word-of-mouth via consumer-opinion platforms: {What} motivates consumers to articulate themselves on the {Internet}?},
  volume     = {18},
  issn       = {1094-9968},
  shorttitle = {Electronic word-of-mouth via consumer-opinion platforms},
  doi        = {10.1002/dir.10073},
  abstract   = {Through Web-based consumer opinion platforms (e.g., epinions.com), the Internet enables customers to share their opinions on, and experiences with, goods and services with a multitude of other consumers; that is, to engage in electronic word-of-mouth (eWOM) communication. Drawing on findings from research on virtual communities and traditional word-of-mouth literature, a typology for motives of consumer online articulation is developed. Using an online sample of some 2,000 consumers, information on the structure and relevance of the motives of consumers’ online articulations is generated. The resulting analysis suggests that consumers’ desire for social interaction, desire for economic incentives, their concern for other consumers, and the potential to enhance their own self-worth are the primary factors leading to eWOM behavior. Further, eWOM providers can be grouped based on what motivates their behavior, suggesting that firms may need to develop different strategies for encouraging eWOM behavior among their users.},
  language   = {eng},
  number     = {1},
  journal    = {Journal of interactive marketing},
  author     = {Hennig-Thurau, Thorsten and Gwinner, Kevin P. and Walsh, Gianfranco and Gremler, Dwayne D.},
  year       = {2004},
  note       = {Place: Hoboken
                Publisher: Elsevier Inc},
  keywords   = {Altruism, College professors, Communication, Community, Consumers, Consumption, Customers, Interactive marketing, Internet, Social interaction, Virtual communities},
  pages      = {38--52}
}

@article{hochreiterLongShortTermMemory1997,
  title    = {Long {Short}-{Term} {Memory}},
  volume   = {9},
  issn     = {0899-7667},
  url      = {https://doi.org/10.1162/neco.1997.9.8.1735},
  doi      = {10.1162/neco.1997.9.8.1735},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  number   = {8},
  urldate  = {2023-08-08},
  journal  = {Neural Computation},
  author   = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  month    = nov,
  year     = {1997},
  pages    = {1735--1780}
}


@article{istanbulluogluComplaintHandlingSocial2017,
  title      = {Complaint handling on social media: {The} impact of multiple response times on consumer satisfaction},
  volume     = {74},
  issn       = {0747-5632},
  shorttitle = {Complaint handling on social media},
  url        = {https://www.sciencedirect.com/science/article/pii/S0747563217302583},
  doi        = {10.1016/j.chb.2017.04.016},
  abstract   = {With the increasing popularity of social media, understanding online consumer behaviors is becoming increasingly important for researchers in this field and practitioners who manage social media accounts. The focus of this study is one of the critical aspects of online complaint handling: response time. Using data collected from consumers who complained on Facebook or Twitter, the study explores how the response times of multiple company responses on social media influence consumer satisfaction. Specifically, the first response after the complaint and the conclusive response that closed the complaint file were investigated in regard to consumers’ objectives for complaining. Participants in the study stated that they expected companies to reply to their complaints within 1–3 h on Twitter and within 3–6 h on Facebook. The analysis reveals that both a quicker first response and a quicker conclusive response lead to higher satisfaction with complaint handling. Furthermore, in contrast to previous research on response time in offline redress-seeking situations, which has suggested that when consumers receive redress, response time does not have an effect on satisfaction, the findings of this study suggest that a speedy response increases satisfaction regardless of the consumers’ objectives. These findings provide implications for researchers and practitioners.},
  urldate    = {2023-08-21},
  journal    = {Computers in Human Behavior},
  author     = {Istanbulluoglu, Doga},
  month      = sep,
  year       = {2017},
  keywords   = {Complaint handling, Consumer complaining behavior, Online complaining, Response time},
  pages      = {72--82},
  file       = {ScienceDirect Full Text PDF:/Users/nitinmathew/Zotero/storage/BRR3YEIW/Istanbulluoglu - 2017 - Complaint handling on social media The impact of .pdf:application/pdf;ScienceDirect Snapshot:/Users/nitinmathew/Zotero/storage/A3LMJ2GB/S0747563217302583.html:text/html}
}


@article{jin_complaint_2020,
  title     = {Complaint {Identification} in {Social} {Media} with {Transformer} {Networks}},
  copyright = {http://arxiv.org/licenses/nonexclusive-distrib/1.0},
  abstract  = {Complaining is a speech act extensively used by humans to communicate a negative inconsistency between reality and expectations. Previous work on automatically identifying complaints in social media has focused on using feature-based and task-specific neural network models. Adapting state-of-the-art pre-trained neural language models and their combinations with other linguistic information from topics or sentiment for complaint prediction has yet to be explored. In this paper, we evaluate a battery of neural models underpinned by transformer networks which we subsequently combine with linguistic information. Experiments on a publicly available data set of complaints demonstrate that our models outperform previous state-of-the-art methods by a large margin achieving a macro F1 up to 87.},
  language  = {eng},
  author    = {Jin, Mali and Aletras, Nikolaos},
  year      = {2020},
  keywords  = {Computer Science - Computation and Language}
}


@article{jinModelingSeverityComplaints2021,
  title     = {Modeling the {Severity} of {Complaints} in {Social} {Media}},
  copyright = {http://arxiv.org/licenses/nonexclusive-distrib/1.0},
  abstract  = {The speech act of complaining is used by humans to communicate a negative mismatch between reality and expectations as a reaction to an unfavorable situation. Linguistic theory of pragmatics categorizes complaints into various severity levels based on the face-threat that the complainer is willing to undertake. This is particularly useful for understanding the intent of complainers and how humans develop suitable apology strategies. In this paper, we study the severity level of complaints for the first time in computational linguistics. To facilitate this, we enrich a publicly available data set of complaints with four severity categories and train different transformer-based networks combined with linguistic information achieving 55.7 macro F1. We also jointly model binary complaint classification and complaint severity in a multi-task setting achieving new state-of-the-art results on binary complaint detection reaching up to 88.2 macro F1. Finally, we present a qualitative analysis of the behavior of our models in predicting complaint severity levels.},
  language  = {eng},
  author    = {Jin, Mali and Aletras, Nikolaos},
  year      = {2021},
  keywords  = {Computer Science - Computation and Language}
}



@article{lauIndividualSituationalFactors2001,
  title     = {Individual and {Situational} {Factors} {Influencing} {Negative} {Word}-of-{Mouth} {Behaviour}},
  volume    = {18},
  copyright = {© 2001 ASAC},
  issn      = {1936-4490},
  url       = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1936-4490.2001.tb00253.x},
  doi       = {10.1111/j.1936-4490.2001.tb00253.x},
  abstract  = {Negative word-of-mouth is one form of consumer response to dissatisfaction that has received Utile attention from business firms, yet it is a silent and potent force that is capable of wreaking havoc on a firrn's bottomline. This study examines the influence of some individual and situational factors affecting negative word-ofmouth behaviour. The results reveal that product involvement, purchase decision involvement, self-confidence, perceived worthiness of complaining, and proximity of others ajfect negative word-of-mouth behaviour in both Singapore and Canada. Two additional factors, attitudes towards business in generai and the perceived reputation of the firrn, affect negative word-of-mouth behaviour in the Singapore sample, while an additional factor, sociability, affects negative word-of-mouth behaviour in the Canadian sample. Résumé Le bouche à oreille négatif est une forme de réponse des consommateurs à leur propre insatisfaction qui a reçu peu d'attention de la part des entreprises, et qui cependant, représente une force silencieuse et puissante, capable de nuire aux succès de l'entreprise. Cette étude examine l'influence de facteurs individuels et situationnels sur les comportements de bouche à oreille négatif. Les résultats révèlent que le degré de contact avec le produit, le degré d'implication dans la décision d'achat, la confiance en soi, la perception qu'a le consommateur du suivi donné à une éventuelle plainte, et l'influence d'autrui affectent les comportements de bouche à oreille négatif, aussi bien à Singapour qu'au Canada. Deux facteurs supplémentaires, l'attitude vis-à-vis du monde des affaires en général et la réputation de l'entreprise, affectent le bouche à oreille négatif dans l'échantillon singapourien, alors qu'un facteur supplémentaire, la sociabilité, influence le bouche à oreille négatif dans l'échantillon canadien.},
  language  = {en},
  number    = {3},
  urldate   = {2023-08-20},
  journal   = {Canadian Journal of Administrative Sciences / Revue Canadienne des Sciences de l'Administration},
  author    = {Lau, Geok Theng and Ng, Sophia},
  year      = {2001},
  note      = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1936-4490.2001.tb00253.x},
  pages     = {163--178},
  file      = {Full Text PDF:/Users/nitinmathew/Zotero/storage/NCEVI7PX/Lau and Ng - 2001 - Individual and Situational Factors Influencing Neg.pdf:application/pdf;Snapshot:/Users/nitinmathew/Zotero/storage/FV43DQI3/j.1936-4490.2001.tb00253.html:text/html}
}



@article{liang_dictionary-based_2006,
  title    = {Dictionary-based text categorization of chemical web pages},
  volume   = {42},
  issn     = {0306-4573},
  url      = {https://www.sciencedirect.com/science/article/pii/S0306457305001160},
  doi      = {https://doi.org/10.1016/j.ipm.2005.09.001},
  abstract = {A new dictionary-based text categorization approach is proposed to classify the chemical web pages efficiently. Using a chemistry dictionary, the approach can extract chemistry-related information more exactly from web pages. After automatic segmentation on the documents to find dictionary terms for document expansion, the approach adopts latent semantic indexing (LSI) to produce the final document vectors, and the relevant categories are finally assigned to the test document by using the k-NN text categorization algorithm. The effects of the characteristics of chemistry dictionary and test collection on the categorization efficiency are discussed in this paper, and a new voting method is also introduced to improve the categorization performance further based on the collection characteristics. The experimental results show that the proposed approach has the superior performance to the traditional categorization method and is applicable to the classification of chemical web pages.},
  number   = {4},
  journal  = {Information Processing \& Management},
  author   = {Liang, Chun-Yan and Guo, Li and Xia, Zhao-Jie and Nie, Feng-Guang and Li, Xiao-Xia and Su, Liang and Yang, Zhang-Yuan},
  year     = {2006},
  keywords = {-NN, Automatic segmentation, Chemistry-focused search engine, Dictionary-based text categorization, Latent semantic indexing, Voting},
  pages    = {1017--1029}
}



@inproceedings{luiLangidPyOfftheshelf2012,
  address   = {Jeju Island, Korea},
  title     = {langid.py: {An} {Off}-the-shelf {Language} {Identification} {Tool}},
  url       = {https://aclanthology.org/P12-3005},
  booktitle = {Proceedings of the {ACL} 2012 {System} {Demonstrations}},
  publisher = {Association for Computational Linguistics},
  author    = {Lui, Marco and Baldwin, Timothy},
  month     = jul,
  year      = {2012},
  pages     = {25--30}
}



@misc{nguyenBERTweetPretrainedLanguage2020,
  title      = {{BERTweet}: {A} pre-trained language model for {English} {Tweets}},
  shorttitle = {{BERTweet}},
  url        = {http://arxiv.org/abs/2005.10200},
  doi        = {10.48550/arXiv.2005.10200},
  abstract   = {We present BERTweet, the first public large-scale pre-trained language model for English Tweets. Our BERTweet, having the same architecture as BERT-base (Devlin et al., 2019), is trained using the RoBERTa pre-training procedure (Liu et al., 2019). Experiments show that BERTweet outperforms strong baselines RoBERTa-base and XLM-R-base (Conneau et al., 2020), producing better performance results than the previous state-of-the-art models on three Tweet NLP tasks: Part-of-speech tagging, Named-entity recognition and text classification. We release BERTweet under the MIT License to facilitate future research and applications on Tweet data. Our BERTweet is available at https://github.com/VinAIResearch/BERTweet},
  urldate    = {2023-08-04},
  publisher  = {arXiv},
  author     = {Nguyen, Dat Quoc and Vu, Thanh and Nguyen, Anh Tuan},
  month      = oct,
  year       = {2020},
  note       = {arXiv:2005.10200 [cs]},
  keywords   = {Computer Science - Computation and Language, Computer Science - Machine Learning},
  annote     = {Comment: In Proceedings of EMNLP 2020: System Demonstrations},
  file       = {arXiv Fulltext PDF:/Users/nitinmathew/Zotero/storage/ZF79EWG9/Nguyen et al. - 2020 - BERTweet A pre-trained language model for English.pdf:application/pdf;arXiv.org Snapshot:/Users/nitinmathew/Zotero/storage/FSRW6IFL/2005.html:text/html}
}


@article{olshtain_speechact_1987,
  title     = {Complaints: {A} study of speech act behavior among native and non-native speakers of {Hebrew}},
  url       = {https://www.jbe-platform.com/content/books/9789027286253-pbcs.5.15ols},
  booktitle = {The {Pragmatic} {Perspective}: {Selected} {Papers} from the 1985 {International} {Pragmatics} {Conference}},
  publisher = {John Benjamins},
  author    = {Olshtain, Elite and Weinbach, Liora},
  year      = {1987},
  pages     = {195--208}
}


@misc{perezPysentimientoPythonToolkit2021,
  title      = {pysentimiento: {A} {Python} {Toolkit} for {Sentiment} {Analysis} and {SocialNLP} tasks},
  shorttitle = {pysentimiento},
  url        = {http://arxiv.org/abs/2106.09462},
  doi        = {10.48550/arXiv.2106.09462},
  abstract   = {Extracting opinions from texts has gathered a lot of interest in the last years, as we are experiencing an unprecedented volume of user-generated content in social networks and other places. A problem that social researchers find in using opinion mining tools is that they are usually behind commercial APIs and unavailable for other languages than English. To address these issues, we present pysentimiento, a multilingual Python toolkit for Sentiment Analysis and other Social NLP tasks. This open-source library brings state-of-the-art models for Spanish and English in a black-box fashion, allowing researchers to easily access these techniques.},
  urldate    = {2023-08-15},
  publisher  = {arXiv},
  author     = {Pérez, Juan Manuel and Giudici, Juan Carlos and Luque, Franco},
  month      = jun,
  year       = {2021},
  note       = {arXiv:2106.09462 [cs]},
  keywords   = {Computer Science - Computation and Language},
  annote     = {Comment: 4 pages, 2 tables Source code at https://github.com/pysentimiento/pysentimiento/ Submitted to ASAI/JAIIO},
  file       = {arXiv Fulltext PDF:/Users/nitinmathew/Zotero/storage/3V7XLPRV/Pérez et al. - 2021 - pysentimiento A Python Toolkit for Sentiment Anal.pdf:application/pdf;arXiv.org Snapshot:/Users/nitinmathew/Zotero/storage/Z8X8RL6X/2106.html:text/html}
}



@article{pfefferUnderstandingOnlineFirestorms2014,
  title      = {Understanding online firestorms: {Negative} word-of-mouth dynamics in social media networks},
  volume     = {20},
  issn       = {1352-7266},
  shorttitle = {Understanding online firestorms},
  url        = {https://doi.org/10.1080/13527266.2013.797778},
  doi        = {10.1080/13527266.2013.797778},
  abstract   = {Social media are, on the one hand, a highly beneficial environment for word-of-mouth (WOM) propagation of new ideas and products, and this has increasingly made them a focus of marketing communications. On the other hand, companies and their brands as well as politicians, governmental institutions, and celebrities have increasingly been facing the impact of negative online WOM and complaint behavior. In reaction to any questionable statement or activity, social media users can create huge waves of outrage within just a few hours. These so-called online firestorms pose new challenges for marketing communications. In this article, we group observations from recent online firestorms, identify related social and economic science theories, and derive generalized factors that form the basis for the proliferation of these dynamics. Furthermore, we discuss the consequences of online firestorms for marketing communications, and offer courses of action for marketers to navigate through crises of negative online WOM.},
  number     = {1-2},
  urldate    = {2023-08-21},
  journal    = {Journal of Marketing Communications},
  author     = {Pfeffer, J. and Zorbach, T. and Carley, K. M.},
  month      = mar,
  year       = {2014},
  note       = {Publisher: Routledge
                \_eprint: https://doi.org/10.1080/13527266.2013.797778},
  keywords   = {complaint behavior, online firestorms, online word-of-mouth, opinion diffusion, social media networks},
  pages      = {117--128},
  file       = {Full Text PDF:/Users/nitinmathew/Zotero/storage/TJUFJZI7/Pfeffer et al. - 2014 - Understanding online firestorms Negative word-of-.pdf:application/pdf}
}


@article{preotiuc-pietro_automatically_2019,
  title     = {Automatically {Identifying} {Complaints} in {Social} {Media}},
  copyright = {http://creativecommons.org/publicdomain/zero/1.0},
  abstract  = {Complaining is a basic speech act regularly used in human and computer mediated communication to express a negative mismatch between reality and expectations in a particular situation. Automatically identifying complaints in social media is of utmost importance for organizations or brands to improve the customer experience or in developing dialogue systems for handling and responding to complaints. In this paper, we introduce the first systematic analysis of complaints in computational linguistics. We collect a new annotated data set of written complaints expressed in English on Twitter.{\textbackslash}footnoteData and code is available here: https://github.com/danielpreotiuc/complaints-social-media We present an extensive linguistic analysis of complaining as a speech act in social media and train strong feature-based and neural models of complaints across nine domains achieving a predictive performance of up to 79 F1 using distant supervision.},
  language  = {eng},
  author    = {Preotiuc-Pietro, Daniel and Gaman, Mihaela and Aletras, Nikolaos},
  year      = {2019}
}


@article{qiuYouAreWhat2012,
  title      = {You are what you tweet: {Personality} expression and perception on {Twitter}},
  volume     = {46},
  issn       = {0092-6566},
  shorttitle = {You are what you tweet},
  url        = {https://www.sciencedirect.com/science/article/pii/S009265661200133X},
  doi        = {10.1016/j.jrp.2012.08.008},
  abstract   = {Microblogging services such as Twitter have become increasingly popular in recent years. However, little is known about how personality is manifested and perceived in microblogs. In this study, we measured the Big Five personality traits of 142 participants and collected their tweets over a 1-month period. Extraversion, agreeableness, openness, and neuroticism were associated with specific linguistic markers, suggesting that personality manifests in microblogs. Meanwhile, eight observers rated the participants’ personality on the basis of their tweets. Results showed that observers relied on specific linguistic cues when making judgments, and could only judge agreeableness and neuroticism accurately. This study provides new empirical evidence of personality expression in naturalistic settings, and points to the potential of utilizing social media for personality research.},
  number     = {6},
  urldate    = {2023-08-21},
  journal    = {Journal of Research in Personality},
  author     = {Qiu, Lin and Lin, Han and Ramsay, Jonathan and Yang, Fang},
  month      = dec,
  year       = {2012},
  keywords   = {Linguistic analysis, Microblogs, Personality, Social media, Twitter},
  pages      = {710--718},
  file       = {ScienceDirect Full Text PDF:/Users/nitinmathew/Zotero/storage/SD55V6YS/Qiu et al. - 2012 - You are what you tweet Personality expression and.pdf:application/pdf;ScienceDirect Snapshot:/Users/nitinmathew/Zotero/storage/4YZS9JDA/S009265661200133X.html:text/html}
}


@article{rookNormativeInfluencesImpulsive1995,
  title    = {Normative {Influences} on {Impulsive} {Buying} {Behavior}},
  volume   = {22},
  issn     = {0093-5301},
  doi      = {10.1086/209452},
  abstract = {Although consumer researchers have investigated impulse buying for nearly 50 years, almost no research has empirically examined its normative aspects. This article presents conceptual and empirical evidence that consumers' normative evaluations (i.e., judgments about the appropriateness of engaging in impulse buying behavior) moderate the relationship between the impulse buying trait and consumers' buying behaviors. Specifically, the relationship between the buying impulsiveness trait and related buying behaviors is significant only when consumers believe that acting on impulse is appropriate. The findings from two studies across student and retail customer samples converge and support the hypothesized moderating role of consumers' normative evaluations.},
  language = {eng},
  number   = {3},
  journal  = {The Journal of consumer research},
  author   = {Rook, Dennis W. and Fisher, Robert J.},
  year     = {1995},
  note     = {Place: CARY
              Publisher: University of Chicago Press},
  keywords = {Social Sciences, Studies, Business, Business \& Economics, Consumer behavior, Buying behavior, Consumer psychology, Consumer research, Consumer spending, Consumers, Impulse buying, Impulsive personality, Impulsiveness, Normativity, Personality traits, Shopping, Sweaters},
  pages    = {305--313}
}


@misc{sanhDistilBERTDistilledVersion2020,
  title      = {{DistilBERT}, a distilled version of {BERT}: smaller, faster, cheaper and lighter},
  shorttitle = {{DistilBERT}, a distilled version of {BERT}},
  url        = {http://arxiv.org/abs/1910.01108},
  doi        = {10.48550/arXiv.1910.01108},
  abstract   = {As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40\%, while retaining 97\% of its language understanding capabilities and being 60\% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.},
  urldate    = {2023-06-29},
  publisher  = {arXiv},
  author     = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  month      = feb,
  year       = {2020},
  note       = {arXiv:1910.01108 [cs]},
  keywords   = {Computer Science - Computation and Language},
  annote     = {Comment: February 2020 - Revision: fix bug in evaluation metrics, updated metrics, argumentation unchanged. 5 pages, 1 figure, 4 tables. Accepted at the 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS 2019},
  file       = {arXiv Fulltext PDF:/Users/nitinmathew/Zotero/storage/ZZSCPCF7/Sanh et al. - 2020 - DistilBERT, a distilled version of BERT smaller, .pdf:application/pdf;arXiv.org Snapshot:/Users/nitinmathew/Zotero/storage/ICWWL7H3/1910.html:text/html}
}


@article{shane-simpsonWhyCollegeStudents2018,
  title      = {Why do college students prefer {Facebook}, {Twitter}, or {Instagram}? {Site} affordances, tensions between privacy and self-expression, and implications for social capital},
  volume     = {86},
  issn       = {0747-5632},
  shorttitle = {Why do college students prefer {Facebook}, {Twitter}, or {Instagram}?},
  url        = {https://www.sciencedirect.com/science/article/pii/S074756321830205X},
  doi        = {10.1016/j.chb.2018.04.041},
  abstract   = {Whereas the bulk of research on social media has taken a granular approach, targeting specific behaviors on one site, or to a lesser extent, multiple sites, the current study aimed to holistically examine the social media landscape, exploring questions about who is drawn to popular social media sites, why they prefer each site, and the social consequences of site preference. Survey data was collected from 663 college students regarding their use and preference for Facebook, Instagram, or Twitter. Results highlight the popularity of Instagram for college students, and women in particular. Personal characteristics such as gender, age, affordances on specific sites, and privacy concerns predicted social media preference. Expanding upon the privacy paradox, we found that participants who preferred Twitter were more likely to have a public (vs. private) profile, reported higher levels of self-disclosure, and indicated more bridging social capital. Participants who preferred Facebook reported lower levels of self-disclosure, but higher levels of bonding social capital compared to those who preferred Instagram. These findings suggest that associations between privacy settings, disclosure, and social capital vary as a function of both user motivations and the affordances of specific social media sites.},
  urldate    = {2023-08-21},
  journal    = {Computers in Human Behavior},
  author     = {Shane-Simpson, Christina and Manago, Adriana and Gaggi, Naomi and Gillespie-Lynch, Kristen},
  month      = sep,
  year       = {2018},
  keywords   = {Disclosure, Facebook, Instagram, Privacy, Social capital, Twitter},
  pages      = {276--288},
  file       = {ScienceDirect Full Text PDF:/Users/nitinmathew/Zotero/storage/PJ68ZG8B/Shane-Simpson et al. - 2018 - Why do college students prefer Facebook, Twitter, .pdf:application/pdf;ScienceDirect Snapshot:/Users/nitinmathew/Zotero/storage/3IVB43CA/S074756321830205X.html:text/html}
}


@article{sharma_complainers_2010,
  title     = {Complainers versus non-complainers: a multi-national investigation of individual and situational influences on customer complaint behaviour},
  volume    = {26},
  copyright = {Copyright 2010 Westburn Publishers Ltd. 2010},
  issn      = {0267-257X},
  abstract  = {One of the least understood areas in customer complaint behaviour (CCB) research is why some customers complain and others do not in similar dissatisfaction situations. Prior research has explored differences in customer characteristics between complainers and non-complainers, but not in association with relevant situational factors. This gap is addressed with a new conceptual framework incorporating two situational variables - customer dissatisfaction and involvement - and two consumer traits - impulsivity and self-monitoring. Several hypotheses about their main and interaction effects are tested in two different contexts, using a survey-based study in three countries (Singapore, South Korea, and the United States). Specifically, it is shown that CCB is positively associated with involvement and impulsivity, and negatively with self-monitoring. Involvement and impulsivity are shown to moderate the association between dissatisfaction and CCB positively, and self-monitoring moderates it negatively. Some implications and directions for future research are also discussed.},
  language  = {eng},
  number    = {1-2},
  journal   = {Journal of marketing management},
  author    = {Sharma, Piyush and Marshall, Roger and Alan Reday, Peter and Na, Woonbong},
  year      = {2010},
  note      = {Place: Helensburg
               Publisher: Taylor \& Francis},
  keywords  = {Analysis, Studies, Complaints, Consumer complaints, Consumer behavior, Customer satisfaction, Correlation analysis, customer complaint behaviour, dissatisfaction, Impulse, impulsivity, International aspects, involvement, self-monitoring},
  pages     = {163--180}
}


@article{sparksComplainingCyberspaceMotives2010,
  title      = {Complaining in {Cyberspace}: {The} {Motives} and {Forms} of {Hotel} {Guests}' {Complaints} {Online}},
  volume     = {19},
  issn       = {1936-8623},
  shorttitle = {Complaining in {Cyberspace}},
  doi        = {10.1080/19368623.2010.508010},
  abstract   = {Traditionally, consumers who have been dissatisfied with service have typically complained to the frontline personnel or to a manager in either a direct (face-to-face, over the phone) manner, indirect by writing, or done nothing but told friends and family of the incident. More recently, the Internet has provided various "new" ways to air a grievance, especially when little might have been done at the point of service failure. With the opportunity to now spread word-of-mouth globally, consumers have the potential to impact the standing of a brand or a firm's reputation. The hotel industry is particularly vulnerable, as an increasing number of bookings are undertaken via the Internet and the decision process is likely to be influenced by what other previous guests might post on many booking-linked sites. We conducted a qualitative study of a key travel site to ascertain the forms and motives of complaints made online about hotels and resorts. 200 web-based consumer complaints were analyzed using NVivo 8 software. Findings revealed that consumers report a wide range of service failures on the Internet. They tell a highly descriptive, persuasive, and credible story, often motivated by altruism or, at the other end of the continuum, by revenge. These stories have the power to influence potential guests to book or not book accommodation at the affected properties. Implications for managers of hotels and resorts are discussed.},
  language   = {eng},
  number     = {7},
  journal    = {Journal of hospitality marketing \& management},
  author     = {Sparks, Beverley A. and Browning, Victoria},
  year       = {2010},
  note       = {Publisher: Taylor \& Francis Group},
  keywords   = {e-complaints, Hospitality, Online complaints, Service failure},
  pages      = {797--818},
  file       = {Full Text:/Users/nitinmathew/Zotero/storage/FZNNG5VM/Sparks and Browning - 2010 - Complaining in Cyberspace The Motives and Forms o.pdf:application/pdf}
}



@article{sunDoesActiveService2021,
  title      = {Does {Active} {Service} {Intervention} {Drive} {More} {Complaints} on {Social} {Media}? {The} {Roles} of {Service} {Quality} and {Awareness}},
  volume     = {38},
  issn       = {0742-1222},
  shorttitle = {Does {Active} {Service} {Intervention} {Drive} {More} {Complaints} on {Social} {Media}?},
  url        = {https://doi.org/10.1080/07421222.2021.1958548},
  doi        = {10.1080/07421222.2021.1958548},
  abstract   = {Despite many advantages of social media as a customer service channel, there is a concern that active service intervention encourages excessive service complaints. Our paper casts doubt on this misconception by examining the dynamics between social media customer complaints and brand service interventions. We find service interventions indeed cause more complaints, yet this increase is driven by service awareness rather than chronic complaining. Due to the publicity and connectivity of social media, customers learn about the new service channel by observing customer service delivery to others—a mechanism that is unique to social media customer service and does not exist for traditional call centers. Importantly, high-quality service reduces future complaints. As a result, proactive customer service is a sound strategy on social media, as long as firms dedicate to service quality. Hence, firms should be less concerned about whether to respond and more focused on how to respond to customer complaints.},
  number     = {3},
  urldate    = {2023-08-20},
  journal    = {Journal of Management Information Systems},
  author     = {Sun, Shujing and Gao, Yang and Rui, Huaxia},
  month      = jul,
  year       = {2021},
  note       = {Publisher: Routledge
                \_eprint: https://doi.org/10.1080/07421222.2021.1958548},
  keywords   = {awareness enhancement, chronic complainer, complaint management, customer service, Social media, Twitter},
  pages      = {579--611},
  file       = {Full Text PDF:/Users/nitinmathew/Zotero/storage/YL5CBMQ5/Sun et al. - 2021 - Does Active Service Intervention Drive More Compla.pdf:application/pdf}
}


@misc{sunMobileBERTCompactTaskAgnostic2020,
  title      = {{MobileBERT}: a {Compact} {Task}-{Agnostic} {BERT} for {Resource}-{Limited} {Devices}},
  shorttitle = {{MobileBERT}},
  url        = {http://arxiv.org/abs/2004.02984},
  doi        = {10.48550/arXiv.2004.02984},
  abstract   = {Natural Language Processing (NLP) has recently achieved great success by using huge pre-trained models with hundreds of millions of parameters. However, these models suffer from heavy model sizes and high latency such that they cannot be deployed to resource-limited mobile devices. In this paper, we propose MobileBERT for compressing and accelerating the popular BERT model. Like the original BERT, MobileBERT is task-agnostic, that is, it can be generically applied to various downstream NLP tasks via simple fine-tuning. Basically, MobileBERT is a thin version of BERT\_LARGE, while equipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks. To train MobileBERT, we first train a specially designed teacher model, an inverted-bottleneck incorporated BERT\_LARGE model. Then, we conduct knowledge transfer from this teacher to MobileBERT. Empirical studies show that MobileBERT is 4.3x smaller and 5.5x faster than BERT\_BASE while achieving competitive results on well-known benchmarks. On the natural language inference tasks of GLUE, MobileBERT achieves a GLUEscore o 77.7 (0.6 lower than BERT\_BASE), and 62 ms latency on a Pixel 4 phone. On the SQuAD v1.1/v2.0 question answering task, MobileBERT achieves a dev F1 score of 90.0/79.2 (1.5/2.1 higher than BERT\_BASE).},
  urldate    = {2023-08-04},
  publisher  = {arXiv},
  author     = {Sun, Zhiqing and Yu, Hongkun and Song, Xiaodan and Liu, Renjie and Yang, Yiming and Zhou, Denny},
  month      = apr,
  year       = {2020},
  note       = {arXiv:2004.02984 [cs]},
  keywords   = {Computer Science - Computation and Language, Computer Science - Machine Learning},
  annote     = {Comment: Accepted to ACL 2020},
  file       = {arXiv Fulltext PDF:/Users/nitinmathew/Zotero/storage/UPVNAA52/Sun et al. - 2020 - MobileBERT a Compact Task-Agnostic BERT for Resou.pdf:application/pdf;arXiv.org Snapshot:/Users/nitinmathew/Zotero/storage/NMDTPAQ6/2004.html:text/html}
}



@article{tripp_when_2011,
  title     = {When unhappy customers strike back on the {Internet}},
  volume    = {52},
  copyright = {Copyright © Massachusetts Institute of Technology, 2011. All rights reserved.},
  issn      = {1532-9194},
  abstract  = {A study of best action for companies to deal with online complainers is presented. The findings show that the triggers of online complaining can be avoided by developing fair processes and a triage system for initial non-online complaints. Companies should also respond to online complaints as quickly as possible and the nature of apology must depend on type of customer involved.},
  language  = {eng},
  number    = {3},
  journal   = {MIT Sloan management review},
  author    = {Tripp, Thomas M and Gregoire, Yany},
  year      = {2011},
  note      = {Place: Cambridge
               Publisher: Sloan Management Review},
  keywords  = {Consumer complaints, Criticism, Customer relations, Customer relationship management, Customer satisfaction, Electronic discussion groups, Forecasts and trends, Information dissemination, Methods, Strategic planning, World Wide Web},
  pages     = {37}
}


@misc{turcWellReadStudentsLearn2019,
  title      = {Well-{Read} {Students} {Learn} {Better}: {On} the {Importance} of {Pre}-training {Compact} {Models}},
  shorttitle = {Well-{Read} {Students} {Learn} {Better}},
  url        = {http://arxiv.org/abs/1908.08962},
  doi        = {10.48550/arXiv.1908.08962},
  abstract   = {Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Due to the cost of applying such models to down-stream tasks, several model compression techniques on pre-trained language representations have been proposed (Sun et al., 2019; Sanh, 2019). However, surprisingly, the simple baseline of just pre-training and fine-tuning compact models has been overlooked. In this paper, we first show that pre-training remains important in the context of smaller architectures, and fine-tuning pre-trained compact models can be competitive to more elaborate methods proposed in concurrent work. Starting with pre-trained compact models, we then explore transferring task knowledge from large fine-tuned models through standard knowledge distillation. The resulting simple, yet effective and general algorithm, Pre-trained Distillation, brings further improvements. Through extensive experiments, we more generally explore the interaction between pre-training and distillation under two variables that have been under-studied: model size and properties of unlabeled task data. One surprising observation is that they have a compound effect even when sequentially applied on the same data. To accelerate future research, we will make our 24 pre-trained miniature BERT models publicly available.},
  urldate    = {2023-08-04},
  publisher  = {arXiv},
  author     = {Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  month      = sep,
  year       = {2019},
  note       = {arXiv:1908.08962 [cs]},
  keywords   = {Computer Science - Computation and Language},
  annote     = {Comment: Added comparison to concurrent work},
  file       = {arXiv Fulltext PDF:/Users/nitinmathew/Zotero/storage/GTRS3K2K/Turc et al. - 2019 - Well-Read Students Learn Better On the Importance.pdf:application/pdf;arXiv.org Snapshot:/Users/nitinmathew/Zotero/storage/AATUTP35/1908.html:text/html}
}


@misc{vaswaniAttentionAllYou2023a,
  title     = {Attention {Is} {All} {You} {Need}},
  url       = {http://arxiv.org/abs/1706.03762},
  doi       = {10.48550/arXiv.1706.03762},
  abstract  = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  urldate   = {2023-08-08},
  publisher = {arXiv},
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  month     = aug,
  year      = {2023},
  note      = {arXiv:1706.03762 [cs]},
  keywords  = {Computer Science - Computation and Language, Computer Science - Machine Learning},
  annote    = {Comment: 15 pages, 5 figures},
  file      = {arXiv Fulltext PDF:/Users/nitinmathew/Zotero/storage/ANXW2JI7/Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:/Users/nitinmathew/Zotero/storage/MZ6M9IFF/1706.html:text/html}
}

@article{hirschbergAdvancesNaturalLanguage2015,
	title = {Advances in natural language processing},
	volume = {349},
	url = {https://www.science.org/doi/full/10.1126/science.aaa8685},
	doi = {10.1126/science.aaa8685},
	abstract = {Natural language processing employs computational techniques for the purpose of learning, understanding, and producing human language content. Early computational approaches to language research focused on automating the analysis of the linguistic structure of language and developing basic technologies such as machine translation, speech recognition, and speech synthesis. Today’s researchers refine and make use of such tools in real-world applications, creating spoken dialogue systems and speech-to-speech translation engines, mining social media for information about health or finance, and identifying sentiment and emotion toward products and services. We describe successes and challenges in this rapidly advancing area.},
	number = {6245},
	urldate = {2023-08-22},
	journal = {Science},
	author = {Hirschberg, Julia and Manning, Christopher D.},
	month = jul,
	year = {2015},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {261--266},
	file = {Full Text PDF:/Users/nitinmathew/Zotero/storage/3NNPPFDB/Hirschberg and Manning - 2015 - Advances in natural language processing.pdf:application/pdf},
}

@article{choPropertiesNeuralMachine2014,
	title = {On the {Properties} of {Neural} {Machine} {Translation}: {Encoder}-{Decoder} {Approaches}},
	shorttitle = {On the {Properties} of {Neural} {Machine} {Translation}},
	url = {https://arxiv.org/abs/1409.1259},
	doi = {10.48550/arxiv.1409.1259},
	abstract = {Neural machine translation is a relatively new approach to statistical
machine translation based purely on neural networks. The neural machine
translation models often consist of an encoder and a decoder. The encoder
extracts a fixed-length representation from a variable-length input sentence,
and the decoder generates a correct translation from this representation. In
this paper, we focus on analyzing the properties of the neural machine
translation using two models; RNN Encoder--Decoder and a newly proposed gated
recursive convolutional neural network. We show that the neural machine
translation performs relatively well on short sentences without unknown words,
but its performance degrades rapidly as the length of the sentence and the
number of unknown words increase. Furthermore, we find that the proposed gated
recursive convolutional network learns a grammatical structure of a sentence
automatically.},
	language = {eng},
	urldate = {2023-08-22},
	author = {Cho, Kyunghyun and van Merrienboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
	year = {2014},
}

@misc{bahdanauNeuralMachineTranslation2016,
	title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
	url = {http://arxiv.org/abs/1409.0473},
	doi = {10.48550/arXiv.1409.0473},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	urldate = {2023-08-23},
	publisher = {arXiv},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	month = may,
	year = {2016},
	note = {arXiv:1409.0473 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: Accepted at ICLR 2015 as oral presentation},
	file = {arXiv Fulltext PDF:/Users/nitinmathew/Zotero/storage/7K5E49B9/Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to .pdf:application/pdf;arXiv.org Snapshot:/Users/nitinmathew/Zotero/storage/BGK9QIB7/1409.html:text/html},
}

@misc{howardUniversalLanguageModel2018,
	title = {Universal {Language} {Model} {Fine}-tuning for {Text} {Classification}},
	url = {http://arxiv.org/abs/1801.06146},
	doi = {10.48550/arXiv.1801.06146},
	abstract = {Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24\% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data. We open-source our pretrained models and code.},
	urldate = {2023-08-07},
	publisher = {arXiv},
	author = {Howard, Jeremy and Ruder, Sebastian},
	month = may,
	year = {2018},
	note = {arXiv:1801.06146 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: ACL 2018, fixed denominator in Equation 3, line 3},
	file = {arXiv Fulltext PDF:/Users/nitinmathew/Zotero/storage/YGH879MW/Howard and Ruder - 2018 - Universal Language Model Fine-tuning for Text Clas.pdf:application/pdf;arXiv.org Snapshot:/Users/nitinmathew/Zotero/storage/NGKPX4UF/1801.html:text/html},
}

@misc{merityPointerSentinelMixture2016,
	title = {Pointer {Sentinel} {Mixture} {Models}},
	url = {https://arxiv.org/abs/1609.07843v1},
	abstract = {Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus.},
	language = {en},
	urldate = {2023-08-23},
	journal = {arXiv.org},
	author = {Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
	month = sep,
	year = {2016},
	file = {Full Text PDF:/Users/nitinmathew/Zotero/storage/9FEJ3784/Merity et al. - 2016 - Pointer Sentinel Mixture Models.pdf:application/pdf},
}

@article{liuRoBERTaRobustlyOptimized2019,
	title = {{RoBERTa}: {A} {Robustly} {Optimized} {BERT} {Pretraining} {Approach}},
	copyright = {http://arxiv.org/licenses/nonexclusive-distrib/1.0},
	abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
	language = {eng},
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	year = {2019},
	keywords = {Computer Science - Computation and Language},
}

@article{chiccoAdvantagesMatthewsCorrelation2020,
	title = {The advantages of the {Matthews} correlation coefficient ({MCC}) over {F1} score and accuracy in binary classification evaluation},
	volume = {21},
	issn = {1471-2164},
	doi = {10.1186/s12864-019-6413-7},
	abstract = {To evaluate binary classifications and their confusion matrices, scientific researchers can employ several statistical rates, accordingly to the goal of the experiment they are investigating. Despite being a crucial issue in machine learning, no widespread consensus has been reached on a unified elective chosen measure yet. Accuracy and F
score computed on confusion matrices have been (and still are) among the most popular adopted metrics in binary classification tasks. However, these statistical measures can dangerously show overoptimistic inflated results, especially on imbalanced datasets.
The Matthews correlation coefficient (MCC), instead, is a more reliable statistical rate which produces a high score only if the prediction obtained good results in all of the four confusion matrix categories (true positives, false negatives, true negatives, and false positives), proportionally both to the size of positive elements and the size of negative elements in the dataset.
In this article, we show how MCC produces a more informative and truthful score in evaluating binary classifications than accuracy and F
score, by first explaining the mathematical properties, and then the asset of MCC in six synthetic use cases and in a real genomics scenario. We believe that the Matthews correlation coefficient should be preferred to accuracy and F
score in evaluating binary classification tasks by all scientific communities.},
	language = {eng},
	number = {1},
	journal = {BMC genomics},
	author = {Chicco, Davide and Jurman, Giuseppe},
	year = {2020},
	note = {Place: England
Publisher: BioMed Central Ltd},
	keywords = {Accuracy, Analysis, Binary classification, Biostatistics, Business metrics, Classification, Confusion, Confusion matrices, Correlation coefficient, Correlation coefficients, Dataset imbalance, Datasets, Evaluation, F1 score, Gene expression, Genomics, Learning algorithms, Machine learning, Mathematical analysis, Matrix methods, Matthews correlation coefficient, Researchers, score, Statistics},
	pages = {6--6},
	file = {Full Text:/Users/nitinmathew/Zotero/storage/ETMUS27I/Chicco and Jurman - 2020 - The advantages of the Matthews correlation coeffic.pdf:application/pdf},
}

