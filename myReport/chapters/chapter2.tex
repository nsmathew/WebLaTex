\chapter{Literature Survey}

\section{The act of complaining}

As per \cite{olshtain_speechact_1987}, the speech act of complaining in the traditional sense can be understood from the perspective of the speaker stating their displeasure or dissatisfaction to a target entity or individual. This is done as a reaction to an unfavourable event that is currently taking place or has already occurred. The authors believe a few preconditions have to be satisfied to result in a complaint being made. This includes the speaker's belief the entity or individual is responsible for the unfavourable outcome and that the speaker in question suffers from the consequences. The result is a verbally expressed complaint.
\newline \newline
This expression of complaint could be carried out in various ways. The speaker might choose to directly communicate their complaints or concerns to the individual or entity, either immediately after the incident or at a later time. They might also voice their grievances to others through word-of-mouth or they could even opt to escalate the issue by involving a third party, such as a consumer advocacy office \cite{sparksComplainingCyberspaceMotives2010}.
\newline\newline
The authors of \cite{olshtain_speechact_1987} further delve into the intentions of the speaker in making the complaint. They argue this is carried out with either the hope of repairing the situation or as a 'Face Threatening Act' \cite{brownPolitenessUniversalsLanguage1987}, with the purpose being to damage the face of the individual or entity against whom the complaint is made. In this scenario, a face-threatening act refers to an action that challenges the reputation of the recipient by going against what the recipient desires. These acts can manifest in a verbal form including with variation in tone or inflection or using non-verbal methods.
\newline \newline
While such complaints could be considered direct complaints as per \cite{boxerSocialDistanceSpeech1993}, the authors highlight the use of indirect complaining in speech. In the case of indirect complaints, the speaker does not attribute responsibility for the cause of the complaint to the individual or entity being addressed. The authors theorise, that an indirect complaint is used to bring about 'solidarity' between speakers, which is contrary to the use of direct complaints. It can serve as a means to initiate conversations and establish temporary connections with others. The scope of the data for this project (described in the subsequent chapter) is primarily focused on direct complaints as they are selected based on tweets being addressed to a brand's customer service handle. However, it is possible that tweets which fall into the category of indirect complaints are also part of the dataset.
\newline \newline
Analysing deeper into which types of customers complain more, \cite{sharma_complainers_2010} have looked at how personality traits like impulsivity and self-monitoring impact customer complaining behaviour. \textit{Impulsivity}, as defined by \cite{rookNormativeInfluencesImpulsive1995}, refers to a consistent inclination of customers to act spontaneously and immediately,  without much reflection or careful consideration of available options or potential consequences. This trait remains relatively stable over time for such customers. \textit{Self-monitoring} is described by  \cite{bechererSelfMonitoringModeratingVariable1978} as the propensity to adjust one's behaviour based on the actions or behaviour of others. High self-monitoring individuals are sensitive to others' expressions and behaviour, relying on social cues for their actions, while low self-monitoring individuals may be influenced by personal traits. From their experiments, \cite{sharma_complainers_2010} concluded that individuals with high impulsiveness tend to complain more than those with low impulsiveness, whereas individuals with high self-monitoring tend to complain less than those with low self-monitoring. These effects tend to be more pronounced in situations where the level of dissatisfaction is high. The level of \textit{involvement} a customer has in a product or service also matters since the more deeply a customer engages with a consumption scenario, their inclination to invest resources like time, effort, and money into addressing or complaining about an unsatisfactory encounter increases \cite{lauIndividualSituationalFactors2001}. The results from \cite{sharma_complainers_2010} also validated the positive influence a consumer's degree of involvement has on their likelihood of engaging in complaining behaviour.


\section{Complaining online}
The act of complaining exists online in various forms and with varying degrees of intensity and this prevalence led to the emergence of third-party organisations that provide online channels for customers' ease and convenience \cite{tripp_when_2011}. Notably, there are complaint websites like complaintsboard.com, review websites like trustpilot.com as well as consumer organisations' sites such as consumeraffairs.com, where customers can share their negative experiences and exchange information with others. The impact of negative word of mouth is quite high due to the ease with which negative reports can rapidly reach millions of people, potentially causing significant harm to a company's brand. Various user-generated content platforms such as YouTube, Twitter, and Facebook serve as spaces for expressing complaints. Brands use these platforms for user engagement and this provides the users with the required visibility to potentially raise or escalate an issue. With numerous such options available online, companies can experience significant repercussions arising from actions taken by dissatisfied customers \cite{tripp_when_2011}.
\newline \newline
Of the 431 online complaints assessed by \cite{tripp_when_2011}, 96\% followed what they call a double deviation. This occurs when customers experience both a product or service failure followed by multiple unsuccessful attempts to resolve the issue, resulting in them feeling they have been violated twice. Such customers then resort to online complaining. Their urge to complain online is driven by how they felt betrayed rather than simply being dissatisfied or with any form of malicious intentions to hinder business operations.
\newline\newline
Complaining online is also associated with electronic word-of-mouth or EWOM, which involves sharing information online with a wider group, and it remains accessible over an extended period while often being anonymous \cite{hennig-thurauElectronicWordofmouthConsumeropinion2004}. This type of communication can take place on various platforms, ranging from official company-sponsored sites to unaffiliated blogs. Among the different forms of EWOM, consumer reviews are particularly noteworthy, as they provide valuable insights about products, whether positive or negative \cite{sparksComplainingCyberspaceMotives2010}. Such Negative electronic WOM (EWOM), can significantly damage a brand's reputation and influence potential customers to seek alternative products or services.

\section{Complaining in social media}
With the increased penetration of social media in the lives of consumers, they now have the ability to express their grievances directly and effectively to service providers using multiple social media platforms \cite{balaji_customer_2015}. Prior to this, a significant portion of dissatisfied customers refrained from lodging complaints due to the perception that the costs associated with complaining far outweighed the benefits linked to resolution \cite{sharma_complainers_2010}. This has had a major impact at the fundamental level on how customers complain across industries. Aside from lodging complaints, consumers use social media channels to publicly vent their anger, a behaviour they formerly exhibited by privately expressing dissatisfaction to friends and family \cite{balaji_customer_2015}.\\

The study carried out by \cite{balaji_customer_2015} investigated the elements that incite customers to participate in complaining activities via social media. They asserted that the act of complaining through social media was shaped by multiple factors. These included perceptions of not being treated fairly and wanting retribution, attributions of causality and the personal identity and traits of the individual. Particularly, they found that distinct factors impact both private and public complaints communicated through social media. They found the level of perceived unfairness in the situation had more impact on customers choosing to publicly complain through social media. Additionally, a positive view of the firm's compensation terms fostered the expectation that complaining increased the chance of resolution. Consequently, dissatisfied customers were more inclined to express their grievances privately to the service provider instead of publicly criticising them on social media.\\

Organisations have also been promoting the use of social media and digital channels as a medium for providing customer support while moving away from conventional contact points like call centres \cite{sunDoesActiveService2021}. The cost of handling per customer on Twitter, for example, is significantly lower when compared with a call centre. It is convenient for the customer as well since they can interact with a brand via the social media handles at their preferred time rather than having to wait for a prolonged time over a phone call. Effectively managing a customer's complaint also has the potential to transform a service failure into a favourable brand encounter. This brings about opportunities to showcase the brand in a positive light among other social media users.\\

The speed and scale of communication on social media is a key factor for opinion propagation online \cite{pfefferUnderstandingOnlineFirestorms2014}. Posts on social media provide a continuous stream of information. When it comes to information that appeals to a wider audience, a considerable number of individuals can be reached swiftly. Consequently, this may lead to a situation where a particular topic gains prominence for a short while, triggering a surge in posts on that topic. The reality of social media demands swift responses from affected organisations, often within hours or minutes \cite{pfefferUnderstandingOnlineFirestorms2014}.\\

\section{Twitter as a medium for self-expression}
Twitter is a microblogging platform with over 540 million users and active users amounting to over 250 million\footnote{\url{https://www.reuters.com/business/media-telecom/musk-says-x-monthly-users-reach-new-high-2023-07-28/}}. It is known for being the fastest social media platform due to its high turnover of information and short message length \cite{istanbulluogluComplaintHandlingSocial2017}. This short and quick communication is influenced by both technical factors and the nature of interactions, where opinions are often simplified to likes or upvotes. Messages are usually limited in length enforced by Twitter's character limit (currently at 280 for most users\footnote{\url{https://developer.twitter.com/en/docs/counting-characters}}). \\

Different communication modalities exist for various social media platforms. Twitter uses short messages, primarily in text but could include images and videos while Instagram has images at its core, and Facebook combines both approaches along with privacy controls \cite{shane-simpsonWhyCollegeStudents2018}. The study performed by \cite{shane-simpsonWhyCollegeStudents2018} found that these varying aspects attract different types of consumers to these platforms. It also found that participants who were primarily Twitter users openly shared content leading to increased \textit{Social Capital}. \textit{Online Social Capital} while often quantifiable from the number of followers or connections and likes or views accumulates from digital interactions, which may or may not align with a user's perceived value in the online space \cite{faucherSocialCapitalOnline2018}. Users utilise such indicators to achieve specific objectives as part of their online social life. Participants favouring Twitter appeared to exhibit stronger individualistic tendencies and gave importance to self-expression and choice. They also showed a greater inclination towards forming and dissolving relationships, and trust in unfamiliar individuals. They also found such users often set their profiles for public visibility while building up social connections \cite{shane-simpsonWhyCollegeStudents2018}.\\

The authors of \cite{qiuYouAreWhat2012} explored how personality affects the content and tone of tweets. Microblogging on Twitter generates an extensive written record of daily behaviour in natural settings due to the active usage of the platform by millions of users. They found tweets to be containing valid linguistic cues to the user's personality. They suggest the linguistic characteristics of the tweets in terms of the length of the messages, variation in functional vocabulary and type of emotion used depends on the personality traits of the individuals including the level of extraversion, agreeableness and level of diligence.

\section{Previous work on automation of complaints identification}
The initial research, delving into a computational linguistic analysis of complaints in social media and their automated identification using ML was carried out by\cite{preotiuc-pietro_automatically_2019}. Beyond creating a publicly accessible and annotated Twitter dataset specifically for complaints, the authors conducted a broad evaluation of the quantitative features associated with complaint data. They investigated the efficacy of unigrams, Linguistic Inquiry and Word Count (LIWC) measures, and word clusters as potential features for the classification task. Furthermore, they explored how sentiment and emotion analysis could enhance the detection of complaints, given the intrinsic link between these factors and the language employed. Their findings demonstrated that models using logistic regression with all the extracted features yielded the best predictive performance, with an F1 score of 0.78. Interestingly, this performance surpassed that of neural network models such as Multi-Layer Perceptron (MLP) and Long Short-Term Memory (LSTM), which the authors contend could be due to the lower training data volume. Moreover, supplementary data collected via distant supervision was used to further enhance the Logistic Regression model's F1 score to 0.79.\\

This line of research was extended by \cite{jin_complaint_2020} where they explored using pre-trained transformer language models \cite{vaswaniAttentionAllYou2023a}, combined with linguistic information. They used BERT, ALBERT and RoBERTa for the assessment. M-BERT or Multimodal BERT was used to check how using additional linguistic information for emotion and topical themes could impact the predictive performance. They found the transformer models performed better than the previous baseline \cite{preotiuc-pietro_automatically_2019} and BERT performed the best with an F1 of 0.87 followed closely by RoBERTa. While M-BERT was competitive, the authors felt there was no benefit from injecting this additional information into the finetuning for the BERT models. They also found the distant supervision approach of \cite{preotiuc-pietro_automatically_2019} had a negative impact on the performance of the transformer models aside from M-BERT. \\

Complaints can be classified into different levels of severity using linguistic pragmatics. The severity depends on the extent to which the complainer's 'Face Threatening Act' is intended to harm the recipient. This categorization offers valuable insights into the motivations of the customers and what organisations can do better in addressing the complaints \cite{jinModelingSeverityComplaints2021}. The study by \cite{jinModelingSeverityComplaints2021} explored the use of transformer models combined with linguistic information to perform a classification task for the severity level. The authors decided on the 4 severity levels of 'Disapproval', 'Accusation', 'Blame' and 'No explicit reproach' as defined by \cite{trosborg2011interlanguage} and enhanced the data from \cite{preotiuc-pietro_automatically_2019} to annotate with the severity levels as the classes. Three baseline models were employed: the majority class, logistic regression with bag-of-words, and BiGRU-Att (a bidirectional GRU model with self-attention), alongside RoBERTa. Additionally, a modified version of M-BERT, termed M-RoBERTa was developed. Various versions of this model were then evaluated, incorporating linguistic data for emotions, topics, and a combination of both. The findings indicated that the M-RoBERTa models delivered the most promising results, suggesting that the inclusion of supplementary linguistic information indeed contributed to enhanced model performance. Following closely, the RoBERTa baseline showcased the next best performance, while the logistic regression and BiGRU-Att models faced more challenges in achieving competitive outcomes.\\

The automatic identification of complaints has been researched within other technology channels like email. In the study \cite{coussement_improving_2008}, the authors investigated methods to detect complaint emails in order to improve an organisation's complaint management capabilities. Their approach involved utilising a combination of document features represented as vectors (weighted term frequencies)  as well as linguistic features. These features aimed to capture linguistic styles as predefined groups, and they also considered word counts within each group. To address the sparseness of the matrix, they applied dimensionality reduction using Latent Semantic Indexing. For classification, the authors employed boosting, a technique that combines the predictive outputs of multiple "weak" classifiers to create a more powerful ensemble of classifiers, expected to provide robust predictive performance \cite{hastieElementsStatisticalLearning2009}. The researchers built a corpus of emails received by a newspaper organisation's call centre. Based on their linguistic analysis, they discovered that complaint emails were more likely to exhibit characteristics such as using the present tense, containing higher word counts, articles, time-related indicators, and negations. In terms of predictive performance, their classification system demonstrated an AUC score ranging from 0.846 to 0.913 across different configurations, indicating its effectiveness.


\section{Natural Language Processing methods}
Computational linguistics or Natural Language Processing (NLP) has evolved into a compelling scientific field with significant research being carried out while having practical usage across both consumer use cases as well business operations. NLP focuses on using computational methods to comprehend, generate, and learn from human language content \cite{hirschbergAdvancesNaturalLanguage2015}. The progress in this field over the last decade and more is attributed to four main factors by \cite{hirschbergAdvancesNaturalLanguage2015}: computing power advancements, access to extensive linguistic data, the success of advanced machine learning techniques, and a more comprehensive understanding of human languages in the construct of social usage. The solutions target to solve problems in areas of machine translation, text generation, summarisation and classification, question answering as well generating business insight from the vast amounts of text available online \cite{hirschbergAdvancesNaturalLanguage2015}.\\

\subsection{Transformer network and inductive transfer learning}
The authors of \cite{vaswaniAttentionAllYou2023a} introduced the 'self-attention' mechanism to build an auto-regressive transformer network made up of repeated encoder and decoder stacks but without recurrence as used by Long-Short Term Memory (LSTM) networks. Self-attention allows the network to identify and arrive at a representation of the sequence of text by looking at the entire input sequence at each step. This mechanism aids the model in developing a better understanding of the input data and the relationships between the various parts of the input.\\

\cite{vaswaniAttentionAllYou2023a} describe the encoder made up of a stack of identical layers and generate for an input sequence $[x_1, ..., x_n]$ a representation sequence $[z_1, ..., z_n]$. This representation form is utilised by the decoder which is also stacked to generate an output sequence as $[y_1, ..., y_m]$. At each stage, the decoder uses the previously generated output sequence to create the next token in the sequence making the model auto-regressive. The authors also introduced the concept of multi-head attention where instead of performing attention just once at each step it is performed multiple times (8 were used in the paper). However, the dimensionality is reduced such that when the final output of all the attention projections is concatenated they have the same dimensionality as single-head attention. This limits any impact on computation cost. The results from their experiments showed this architecture had a performance advantage over previous BLEU\footnote{Bilingual Evaluation Understudy, an evaluation method for machine translation of human language.} benchmark scores for English-to-German and English-to-French translation tasks. They also showed significant savings in terms of training computation cost due to the level of parallelisation that was achieved \cite{vaswaniAttentionAllYou2023a}. \\

Inductive transfer is a transfer learning technique in machine learning to leverage the training process and apply learnings and representations from one task onto a different task that has a different data distribution \cite{howardUniversalLanguageModel2018}. Considering the limited success of transfer learning in NLP tasks, \cite{howardUniversalLanguageModel2018} identified key challenges, reflecting on past research.  While computer vision tasks had found success in implementing inductive transfer learning, the authors felt some of the issues faced by NLP tasks were induced by researchers not appreciating the differences between the two task categories. Recognizing potential benefits in tailoring approaches, they proposed a method that pre-trained a language model on Wikitext-103 \cite{merityPointerSentinelMixture2016} representing the general domain dataset. This was followed by fine-tuning the language model using data from the various domains scoped for their experiments. For their network, they decided on 2 layers for task-specific classification which were fine-tuned using data from the target task's domain. Their approach was able to achieve state-of-the-art results on tasks for sentiment analysis, question answering and topic classification.\\

\subsection{BERT and its variants}
Bidirectional Encoder Representations from Transformers or BERT was proposed by \cite{devlinBERTPretrainingDeep2018} to solve some of the issues with the transformer networks due to their use of unidirectional language models during the pre-training phase which limits the strengths of pre-trained representations. They proposed using two pre-training tasks. The first was a masked language model or MLM where a token has been randomly masked and the objective is to predict based on the context. They argued this would assist in pre-training deeper and bi-directional transformer models. The second task involved pre-training sequence pairs and was termed as next sentence prediction or NSP. They showed that using very large model sizes could also result in notable enhancements for even smaller downstream tasks, as long as the model was adequately pre-trained. Their experiments were run with 2 versions of BERT, BERT Base with 110M parameters and BERT Large with 340M parameters. The BERT models demonstrated superior performance compared to other state-of-the-art models across various benchmarks encompassing tasks related to natural language comprehension, inference, and question-answering.\\

With the significant success of the BERT transformer model, several derived models were proposed over the next few years with some of them focusing on improving the pre-training while others were on reducing the model size. Some of the important ones are covered in brief. RoBERTa or Robustly Optimised BERT Pretraining Approach was introduced by \cite{liuRoBERTaRobustlyOptimized2019} to overcome what the authors felt were shortcomings in BERT's \cite{devlinBERTPretrainingDeep2018} pretraining strategy including certain design choices. They made changes in terms of learning rate warmup and tuning the Adam optimiser. They also increased the pretraining with an additional corpus. With these changes, the model achieved state-of-the-art benchmark results.\\

While larger models with appropriate pretraining have better predictive performance \cite{devlinBERTPretrainingDeep2018, liuRoBERTaRobustlyOptimized2019} there exist challenges posed by memory limitations and training times associated with model size increases. A Lite BERT or ALBERT was introduced by \cite{lanALBERTLiteBERT2020} to include techniques to enhance the efficiency of large-scale pre-trained language models. ALBERT employed two methods: factorised embedding parameterisation and cross-layer parameter sharing. The former divided the embedding matrix into smaller components, allowing hidden layers to expand without a substantial rise in vocabulary embedding parameters. The latter controlled parameter growth as network depth increased. These approaches significantly reduced parameters while maintaining performance. ALBERT with BERT Large like configuration has 18 times fewer parameters and 1.7 times faster training time. Their experiments showed ALBERT XXLarge, with about 70\% of BERT Large's parameters, had significant improvements compared to BERT Large across benchmark downstream tasks with speedup achieved in training times as well.\\

DistilBERT \cite{sanhDistilBERTDistilledVersion2020}, a variation of BERT with the knowledge distillation technique applied in the pretraining phase was another model proposed to address similar challenges. Knowledge distillation involves distilling the learnings of a larger 'teacher' model (or ensemble) to a smaller and compact 'student' model as part of training and hence is a form of a compression technique \cite{sanhDistilBERTDistilledVersion2020}. The results from the experiments showed this model had 40\% fewer parameters than BERT Base but was able to achieve performance on downstream benchmark tasks within just 0.6\% to 3.9\% points behind BERT Base. Another goal of the authors was a reduction in inference time, which they achieved by the model being faster by up to 60\%.\\

To develop models that align to pre-defined memory and computational speed requirements, \cite{turcWellReadStudentsLearn2019} proposed various versions of BERT with changes in the model size and embeddings size. They used various strategies to train a student compact model, combined with a bigger teacher model. The smallest model was Transformer Tiny (BERT Tiny) with  4.4M parameters while the largest was BERT Base with 110M parameters. They used various techniques to develop the compact model and compared them with BERT Large as the baseline on 3 NLP tasks. Pretraining the compact model with a masked language model objective and following it with knowledge distillation provided the best performance. This approach was superior to using distillation alone or relying on pretraining combined with fine-tuning.\\

To conclude this section, an approach of pre-training BERT on English tweets to enhance performance on Twitter data is examined. BERTweet Base was introduced by \cite{nguyenBERTweetPretrainedLanguage2020} as the first pre-trained model on large-scale tweets dataset in the public domain. While retaining the BERT Base architecture they used the pre-training optimisations introduced by RoBERTa \cite{liuRoBERTaRobustlyOptimized2019}. They argued the linguistic attributes of tweets like length, grammar and vocabulary differ from the text present in the corpora used for pretraining BERT. 850M tweets were used to pretrain BERTweet. This was followed by fine-tuning the model independently for each of the 3 downstream Twitter NLP tasks. Their results showed the model did better than RoBERTa Base in all 3 NLP tasks. While RoBERTa Large did outperform BERTweet Base on the POS tagging task, they speculate this could be attributed to the significant difference in model sizes.

\subsection{Ongoing research}
GPT or Generative Pre-trained Transformer models have significantly changed the landscape in terms of the general perception of AI as well as its applicability. As per OpenAI\footnote{https://openai.com/research/overview}, the developers of ChatGPT and other GPT-[2,3,3.5,4] versions, their models have undergone training to comprehend both natural language and code. These models generate textual responses based on their inputs. The inputs given to GPT models are commonly referred to as 'prompts'. GPTs enable the development of applications for tasks such as writing documents, coding, answering questions, text analysis, creating dialogue agents and language translation among others\footnote{\url{https://platform.openai.com/docs/guides/gpt}}. The use of prompts allows for performing text classification tasks as well. Some of the recent research on such models will be discussed next.\\

When using transformers, one of the primary drawbacks is that despite the task-agnostic nature, datasets relevant to that task along with fine-tuning are still essential for good performance \cite{brownLanguageModelsAre2020}. This dependency on extensive labelled datasets limits the wider application of language models. The authors contend the aspiration for NLP systems is to eventually achieve versatility and broadness similar to that of humans who typically require minimally supervised datasets to grasp most language tasks. Meta-learning, where a language model learns a wide skill set during training and applies them at inference to adapt quickly to tasks is an approach researched in recent years \cite{brownLanguageModelsAre2020}. Despite some initial promise, this approach lags fine-tuning in terms of results.\\

Another promising direction involves increasing the size of the transformer language models, leading to enhanced NLP performance \cite{brownLanguageModelsAre2020}. The authors argue due to the nature of meta-learning, larger models might exhibit significant improvements in in-context learning abilities. To validate this, the researchers built GPT-3 \cite{brownLanguageModelsAre2020}, an auto-regressive model with 175 billion parameters. GPT-3 was evaluated across three conditions, 'few-shot learning' with multiple demonstrations within the context window, 'one-shot learning' with a single demonstration, and 'zero-shot' learning, relying solely on natural language directions. The model demonstrated good performance across NLP tasks and benchmarks with the 3 evaluation conditions. It closely rivalled state-of-the-art fine-tuned systems in certain cases. For ad-hoc tasks, the model exhibited high qualitative results. Finally, the authors assessed the social impact of using such models and highlighted the models show varying levels of bias across race, religion and gender. They feel this is likely arising from the inherent stereotypes captured within the training data sourced from the Internet \cite{brownLanguageModelsAre2020}.\\

Capable of processing both images and text as input, GPT-4 \cite{openaiGPT4TechnicalReport2023} is a large transformer-based multimodal model, that generates text as its output. It is pre-trained to predict the next token as the output. Such models have a broad area of application with machine translation, text summarisation and dialogue systems as some of the main use cases. The authors \cite{openaiGPT4TechnicalReport2023} claim the strength of GPT-4 is in understanding and generating natural language in intricate and complicated situations and show it performs highly in exams designed for humans. While the model is superior in performance to other state-of-the-art systems in both English and other languages it suffers from limitations including 'hallucinations', inability to learn from experience,  restricted context and inherent bias. While GPT-4 alleviates the hallucination problem when compared to GPT-3.5 (successor to GPT-3), authors suggest usage with caution, especially in mission-critical scenarios. GPT-4 can handle prompts containing both images and text, allowing users to define various vision or language tasks. The model generates text outputs based on mixed text and image inputs, displaying comparable proficiency across different domains as it does with text-only inputs \cite{openaiGPT4TechnicalReport2023}. \\

While the model is extremely versatile and outperforms other state-of-the-art models in tests, the authors acknowledge the risks associated with the higher capabilities and are working with other researchers on recommendations on how society can prepare itself for any potential social and economic impacts from the wider usage of such AI technologies \cite{openaiGPT4TechnicalReport2023}.

