\relax 
\providecommand \oddpage@label [2]{}
\citation{preotiuc-pietro_automatically_2019}
\citation{preotiuc-pietro_automatically_2019}
\citation{bhargavaGeneralizationNLIWays2021}
\citation{nguyenBERTweetPretrainedLanguage2020}
\citation{sanhDistilBERTDistilledVersion2020}
\citation{jin_complaint_2020}
\citation{jinModelingSeverityComplaints2021}
\citation{olshtain_speechact_1987}
\citation{brownPolitenessUniversalsLanguage1987}
\citation{tripp_when_2011}
\citation{balaji_customer_2015}
\citation{preotiuc-pietro_automatically_2019}
\citation{liang_dictionary-based_2006}
\citation{coussement_improving_2008}
\citation{preotiuc-pietro_automatically_2019}
\citation{jin_complaint_2020}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Background}{1}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces Sample complaints extracted from Twitter, exhibiting diverse degrees of complaint expression and severity. These complaints are sourced from data that has undergone the preprocessing steps outlined in Chapter 3.\relax }}{2}{}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab: ex_complaints}{{1.1}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Objectives}{2}{}\protected@file@percent }
\citation{olshtain_speechact_1987}
\citation{sparksComplainingCyberspaceMotives2010}
\citation{olshtain_speechact_1987}
\citation{brownPolitenessUniversalsLanguage1987}
\citation{boxerSocialDistanceSpeech1993}
\citation{sharma_complainers_2010}
\citation{rookNormativeInfluencesImpulsive1995}
\citation{bechererSelfMonitoringModeratingVariable1978}
\citation{sharma_complainers_2010}
\citation{lauIndividualSituationalFactors2001}
\citation{sharma_complainers_2010}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Literature Survey}{4}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}The act of complaining}{4}{}\protected@file@percent }
\citation{tripp_when_2011}
\citation{tripp_when_2011}
\citation{tripp_when_2011}
\citation{hennig-thurauElectronicWordofmouthConsumeropinion2004}
\citation{sparksComplainingCyberspaceMotives2010}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Complaining online}{5}{}\protected@file@percent }
\citation{balaji_customer_2015}
\citation{sharma_complainers_2010}
\citation{balaji_customer_2015}
\citation{balaji_customer_2015}
\citation{sunDoesActiveService2021}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Complaining in social media}{6}{}\protected@file@percent }
\citation{pfefferUnderstandingOnlineFirestorms2014}
\citation{pfefferUnderstandingOnlineFirestorms2014}
\citation{istanbulluogluComplaintHandlingSocial2017}
\citation{shane-simpsonWhyCollegeStudents2018}
\citation{shane-simpsonWhyCollegeStudents2018}
\citation{faucherSocialCapitalOnline2018}
\citation{shane-simpsonWhyCollegeStudents2018}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Twitter as a medium for self-expression}{7}{}\protected@file@percent }
\citation{qiuYouAreWhat2012}
\citation{preotiuc-pietro_automatically_2019}
\citation{jin_complaint_2020}
\citation{vaswaniAttentionAllYou2023a}
\citation{preotiuc-pietro_automatically_2019}
\citation{preotiuc-pietro_automatically_2019}
\citation{jinModelingSeverityComplaints2021}
\citation{jinModelingSeverityComplaints2021}
\citation{trosborg2011interlanguage}
\citation{preotiuc-pietro_automatically_2019}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Previous work on automation of complaints identification}{8}{}\protected@file@percent }
\citation{coussement_improving_2008}
\citation{hastieElementsStatisticalLearning2009}
\citation{hirschbergAdvancesNaturalLanguage2015}
\citation{hirschbergAdvancesNaturalLanguage2015}
\citation{hirschbergAdvancesNaturalLanguage2015}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Natural Language Processing methods}{9}{}\protected@file@percent }
\citation{vaswaniAttentionAllYou2023a}
\citation{vaswaniAttentionAllYou2023a}
\citation{vaswaniAttentionAllYou2023a}
\citation{howardUniversalLanguageModel2018}
\citation{howardUniversalLanguageModel2018}
\citation{merityPointerSentinelMixture2016}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}Transformer network and inductive transfer learning}{10}{}\protected@file@percent }
\citation{devlinBERTPretrainingDeep2018}
\citation{liuRoBERTaRobustlyOptimized2019}
\citation{devlinBERTPretrainingDeep2018}
\citation{devlinBERTPretrainingDeep2018}
\citation{liuRoBERTaRobustlyOptimized2019}
\citation{lanALBERTLiteBERT2020}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}BERT and its variants}{11}{}\protected@file@percent }
\citation{sanhDistilBERTDistilledVersion2020}
\citation{sanhDistilBERTDistilledVersion2020}
\citation{turcWellReadStudentsLearn2019}
\citation{nguyenBERTweetPretrainedLanguage2020}
\citation{liuRoBERTaRobustlyOptimized2019}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.3}Ongoing research}{12}{}\protected@file@percent }
\citation{brownLanguageModelsAre2020}
\citation{brownLanguageModelsAre2020}
\citation{brownLanguageModelsAre2020}
\citation{brownLanguageModelsAre2020}
\citation{brownLanguageModelsAre2020}
\citation{openaiGPT4TechnicalReport2023}
\citation{openaiGPT4TechnicalReport2023}
\citation{openaiGPT4TechnicalReport2023}
\citation{openaiGPT4TechnicalReport2023}
\citation{preotiuc-pietro_automatically_2019}
\citation{shane-simpsonWhyCollegeStudents2018}
\citation{preotiuc-pietro_automatically_2019}
\citation{jin_complaint_2020}
\citation{jinModelingSeverityComplaints2021}
\citation{preotiuc-pietro_automatically_2019}
\citation{preotiuc-pietro_automatically_2019}
\citation{luiLangidPyOfftheshelf2012}
\citation{preotiuc-pietro_automatically_2019}
\citation{preotiuc-pietro_automatically_2019}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Methodology}{15}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Task}{15}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Data and pre-processing}{15}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Criteria for tweets}{15}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Data extraction}{15}{}\protected@file@percent }
\citation{olshtain_speechact_1987}
\citation{artsteinInterCoderAgreementComputational2008}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces The nine domains and the distribution of tweets that are complaints and those that are not. The percentages indicate how the splits are distributed \cite  {preotiuc-pietro_automatically_2019}.\relax }}{16}{}\protected@file@percent }
\newlabel{tab: domains}{{3.1}{16}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Selection of tweets based on random sampling and where they have received replies when addressed to the 93 customer service handles combined with random sampled tweets that are addressed to other handles (random\_reply) and tweets that are not addressed to any handle (random\_tweet) \cite  {preotiuc-pietro_automatically_2019}.\relax }}{16}{}\protected@file@percent }
\newlabel{tab: tweet_counts}{{3.2}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Annotation}{16}{}\protected@file@percent }
\citation{wolfHuggingFaceTransformersStateoftheart2020}
\citation{devlinBERTPretrainingDeep2018}
\citation{vaswaniAttentionAllYou2023a}
\citation{chungEmpiricalEvaluationGated2014}
\citation{hochreiterLongShortTermMemory1997}
\citation{vaswaniAttentionAllYou2023a}
\citation{liuRoBERTaRobustlyOptimized2019}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Environment}{17}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Hardware}{17}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Software}{17}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces Software and library versions used for this project.\relax }}{17}{}\protected@file@percent }
\newlabel{tab: libs_used}{{3.3}{17}}
\citation{bhargavaGeneralizationNLIWays2021}
\citation{nguyenBERTweetPretrainedLanguage2020}
\citation{sanhDistilBERTDistilledVersion2020}
\citation{bhargavaGeneralizationNLIWays2021}
\citation{nguyenBERTweetPretrainedLanguage2020}
\citation{sanhDistilBERTDistilledVersion2020}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Model selection}{18}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3.4}{\ignorespaces The transformer models used for the experiments along with the type of tokenization, and vocabulary size and sorted by the number of parameters for each of them. The parameter counts are from \cite  {bhargavaGeneralizationNLIWays2021} for RoBERTa Base, BERT Base, ALBERT Base and BERT Tiny. For BERTweet it is from \cite  {nguyenBERTweetPretrainedLanguage2020}, and DistilBERT from \cite  {sanhDistilBERTDistilledVersion2020}. Vocabulary sizes and tokenizer types are based on documentation at \texttt  {https://huggingface.co/}.\relax }}{18}{}\protected@file@percent }
\newlabel{tab: model_dtls}{{3.4}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces A summary of the BERT Base (left) and BERT Tiny (right) models for comparison in terms of the layers used and number of parameters. This is based on the models from Hugging Face.\relax }}{19}{}\protected@file@percent }
\newlabel{fig: model_summary}{{3.1}{19}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Data tokenisation}{19}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Choice of settings}{19}{}\protected@file@percent }
\citation{preotiuc-pietro_automatically_2019}
\citation{jinModelingSeverityComplaints2021}
\citation{preotiuc-pietro_automatically_2019}
\citation{jinModelingSeverityComplaints2021}
\newlabel{fig: token_hist}{{3.2a}{20}}
\newlabel{sub@fig: token_hist}{{a}{20}}
\newlabel{fig: token_pp_hist}{{3.2b}{20}}
\newlabel{sub@fig: token_pp_hist}{{b}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces The token count distribution for the full dataset of 3,449 tweets before and after tokenization with the red dashed line indicating 95\% coverage of tweets. BertTokenizer is used here.\relax }}{20}{}\protected@file@percent }
\newlabel{fig: bef_aft_token}{{3.2}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Tokenisation example}{20}{}\protected@file@percent }
\citation{preotiuc-pietro_automatically_2019}
\citation{preotiuc-pietro_automatically_2019}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Experiment set 1: Predictive performance comparison of BERT variants}{21}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3.5}{\ignorespaces The choice of key parameters and hyperparameters used for experiment set 1.\relax }}{22}{}\protected@file@percent }
\newlabel{tab: exp1_params}{{3.5}{22}}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Experiment set 2: Cross-domain predictive performance comparison}{22}{}\protected@file@percent }
\citation{preotiuc-pietro_automatically_2019}
\citation{jinModelingSeverityComplaints2021}
\@writefile{lot}{\contentsline {table}{\numberline {3.6}{\ignorespaces The choice of key parameters and hyperparameters used for experiment set 2. Refer to the Chapter on results for the model and learning rate used for this experiment. \relax }}{23}{}\protected@file@percent }
\newlabel{tab: exp2_params}{{3.6}{23}}
\@writefile{toc}{\contentsline {section}{\numberline {3.8}Ethical, professional and legal issues}{23}{}\protected@file@percent }
\citation{preotiuc-pietro_automatically_2019}
\citation{jinModelingSeverityComplaints2021}
\citation{preotiuc-pietro_automatically_2019}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Results and discussion}{25}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Data exploration}{25}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Domain and class distribution}{25}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Illustrates the distribution of tweets categorised as 'complaints' and 'not complaints', with random 'tweets/replies' shown separately.\relax }}{25}{}\protected@file@percent }
\newlabel{fig: compl_non_random_dist}{{4.1}{25}}
\newlabel{fig: domain_dist_pct}{{4.2a}{26}}
\newlabel{sub@fig: domain_dist_pct}{{a}{26}}
\newlabel{fig: domain_dist_count}{{4.2b}{26}}
\newlabel{sub@fig: domain_dist_count}{{b}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Shows the distribution of the domains used in the dataset\relax }}{26}{}\protected@file@percent }
\newlabel{fig: compl_main_dist}{{4.2}{26}}
\newlabel{fig: top_ngrams}{{4.3a}{26}}
\newlabel{sub@fig: top_ngrams}{{a}{26}}
\newlabel{fig: top_hashtags}{{4.3b}{26}}
\newlabel{sub@fig: top_hashtags}{{b}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Top phrases as n-grams(excl. unigrams) and top hashtags in the complaint tweets.\relax }}{26}{}\protected@file@percent }
\newlabel{fig: top_ngrams_hashtags}{{4.3}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Linguistic analysis}{26}{}\protected@file@percent }
\citation{perezPysentimientoPythonToolkit2021}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Sentiment analysis}{28}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Distribution of positive, negative and neutral sentiments in the tweets.\relax }}{28}{}\protected@file@percent }
\newlabel{fig: sentiment}{{4.4}{28}}
\citation{nguyenBERTweetPretrainedLanguage2020}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.4}Key statistics}{29}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Statistics of tweets in the dataset.\relax }}{29}{}\protected@file@percent }
\newlabel{tab: tweets_statistics}{{4.1}{29}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Experiment set 1 results: Predictive performance comparison of BERT variants}{29}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Best predictive performance}{29}{}\protected@file@percent }
\citation{jin_complaint_2020}
\citation{jin_complaint_2020}
\citation{jin_complaint_2020}
\citation{turcWellReadStudentsLearn2019}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Mean prediction performance metrics for all models after nested cross-validation for finetuning and testing. The highest scores are in bold. \(\uparrow \) is the best performing and \(\downarrow \) is the worst performing model. \(\star \) models are included for deep-dive analysis. Where available, numbers in '[ ]' are the results from \cite  {jin_complaint_2020}.\relax }}{30}{}\protected@file@percent }
\newlabel{tab: model_mean_metrics}{{4.2}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Performance of smaller models}{30}{}\protected@file@percent }
\citation{sanhDistilBERTDistilledVersion2020}
\citation{lanALBERTLiteBERT2020}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Comparison of key metrics of the models relative to BERTweet.\relax }}{31}{}\protected@file@percent }
\newlabel{tab: relative_comparison_metrics}{{4.3}{31}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Relative predictive performance (F1) of models against BERTweet and model sizes. BERTweet's model size is 110M.\relax }}{31}{}\protected@file@percent }
\newlabel{fig: model_size_vs_perf}{{4.5}{31}}
\@writefile{toc}{\contentsline {subsubsection}{Analyis of finetuning and inference time}{32}{}\protected@file@percent }
\newlabel{fig: mean_train_time}{{4.6a}{32}}
\newlabel{sub@fig: mean_train_time}{{a}{32}}
\newlabel{fig: mean_inf_time}{{4.6b}{32}}
\newlabel{sub@fig: mean_inf_time}{{b}{32}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Mean time taken (in seconds) for finetuning and inference during experiment set 1. BERTweet with the best predictive model is highlighted in red.\relax }}{32}{}\protected@file@percent }
\newlabel{fig: mean_time_taken}{{4.6}{32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Deep-dive into the results}{32}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces Test loss from the inference phase for the 6 outer loop iterations for the 3 selected models.\relax }}{33}{}\protected@file@percent }
\newlabel{tab: test_loss}{{4.4}{33}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Confusion matrix and performance metrics for the 3 selected models from the inference phase. The confusion matrix is based on the mean of values from the 6 outer loop iterations and the grid can be read from left to right as true negative, false positive, false negative and true positive.\relax }}{33}{}\protected@file@percent }
\newlabel{fig: deep_dive_results}{{4.7}{33}}
\@writefile{lot}{\contentsline {table}{\numberline {4.5}{\ignorespaces Misclassified sample tweets. Tweets in the \colorbox {Mercury}{lighter shade of grey} are misclassified as complaints while the rest are misclassified as not complaints.\relax }}{35}{}\protected@file@percent }
\newlabel{tab: error_tweets}{{4.5}{35}}
\@writefile{lot}{\contentsline {table}{\numberline {4.6}{\ignorespaces AUC and F1 scores for experiment set 2. The rows show the domain used for finetuning and the columns the domains used for testing. The last row shows the scores where the full data except the corresponding test domain was used for finetuning. Best scores where applicable are highlighted in bold.\relax }}{35}{}\protected@file@percent }
\newlabel{tab: cross_domain_results}{{4.6}{35}}
\citation{jin_complaint_2020}
\citation{jin_complaint_2020}
\citation{jin_complaint_2020}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Experiment set 2 results: Cross-domain predictive performance comparison}{36}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Shows a plot of the number of tweets for each domain against the average performance when that domain is used for finetuning.\relax }}{37}{}\protected@file@percent }
\newlabel{fig: cross_domain_train_domain}{{4.8}{37}}
\citation{jinModelingSeverityComplaints2021}
\bibstyle{acm}
\bibdata{mybibliography}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Conclusions}{38}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{artsteinInterCoderAgreementComputational2008}{1}
\bibcite{balaji_customer_2015}{2}
\bibcite{bechererSelfMonitoringModeratingVariable1978}{3}
\bibcite{bhargavaGeneralizationNLIWays2021}{4}
\bibcite{boxerSocialDistanceSpeech1993}{5}
\bibcite{brownPolitenessUniversalsLanguage1987}{6}
\bibcite{brownLanguageModelsAre2020}{7}
\bibcite{chungEmpiricalEvaluationGated2014}{8}
\bibcite{coussement_improving_2008}{9}
\bibcite{devlinBERTPretrainingDeep2018}{10}
\bibcite{faucherSocialCapitalOnline2018}{11}
\bibcite{hastieElementsStatisticalLearning2009}{12}
\bibcite{hennig-thurauElectronicWordofmouthConsumeropinion2004}{13}
\bibcite{hirschbergAdvancesNaturalLanguage2015}{14}
\bibcite{hochreiterLongShortTermMemory1997}{15}
\bibcite{howardUniversalLanguageModel2018}{16}
\bibcite{istanbulluogluComplaintHandlingSocial2017}{17}
\bibcite{jin_complaint_2020}{18}
\bibcite{jinModelingSeverityComplaints2021}{19}
\bibcite{lanALBERTLiteBERT2020}{20}
\bibcite{lauIndividualSituationalFactors2001}{21}
\bibcite{liang_dictionary-based_2006}{22}
\bibcite{liuRoBERTaRobustlyOptimized2019}{23}
\bibcite{luiLangidPyOfftheshelf2012}{24}
\bibcite{merityPointerSentinelMixture2016}{25}
\bibcite{nguyenBERTweetPretrainedLanguage2020}{26}
\bibcite{olshtain_speechact_1987}{27}
\bibcite{openaiGPT4TechnicalReport2023}{28}
\bibcite{pfefferUnderstandingOnlineFirestorms2014}{29}
\bibcite{preotiuc-pietro_automatically_2019}{30}
\bibcite{perezPysentimientoPythonToolkit2021}{31}
\bibcite{qiuYouAreWhat2012}{32}
\bibcite{rookNormativeInfluencesImpulsive1995}{33}
\bibcite{sanhDistilBERTDistilledVersion2020}{34}
\bibcite{shane-simpsonWhyCollegeStudents2018}{35}
\bibcite{sharma_complainers_2010}{36}
\bibcite{sparksComplainingCyberspaceMotives2010}{37}
\bibcite{sunDoesActiveService2021}{38}
\bibcite{tripp_when_2011}{39}
\bibcite{trosborg2011interlanguage}{40}
\bibcite{turcWellReadStudentsLearn2019}{41}
\bibcite{vaswaniAttentionAllYou2023a}{42}
\bibcite{wolfHuggingFaceTransformersStateoftheart2020}{43}
\@writefile{toc}{\contentsline {chapter}{Appendices}{43}{}\protected@file@percent }
\citation{jinModelingSeverityComplaints2021}
\citation{jinModelingSeverityComplaints2021}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Other supporting analysis and graphs}{44}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Breakdown of tweets in full dataset}{44}{}\protected@file@percent }
\newlabel{sec: apdxa_fulldataset}{{A.1}{44}}
\@writefile{lot}{\contentsline {table}{\numberline {A.1}{\ignorespaces The nine domains and the distribution of tweets that are complaints and those that are not, from the latest version of the dataset available in the public domain and used for the experiments. Additionally, the table includes the number of random tweets and replies introduced into the dataset by the authors for a more proportionate representation of the classes.\relax }}{44}{}\protected@file@percent }
\newlabel{tab: fulldataset_breakdown}{{A.1}{44}}
\@writefile{toc}{\contentsline {section}{\numberline {A.2}Sample data from dataset}{44}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {A.2}{\ignorespaces Sample data from \cite  {jinModelingSeverityComplaints2021}.\relax }}{44}{}\protected@file@percent }
\newlabel{tab: apdx_sample_data}{{A.2}{44}}
\@writefile{toc}{\contentsline {section}{\numberline {A.3}Token distribution after tokenization}{45}{}\protected@file@percent }
\newlabel{sec: apdxa_token_dist}{{A.3}{45}}
\newlabel{fig: token_pp_hist_albert}{{A.1a}{45}}
\newlabel{sub@fig: token_pp_hist_albert}{{a}{45}}
\newlabel{fig: token_pp_hist_bertwteet}{{A.1b}{45}}
\newlabel{sub@fig: token_pp_hist_bertwteet}{{b}{45}}
\newlabel{fig: token_pp_hist_tiny}{{A.1c}{45}}
\newlabel{sub@fig: token_pp_hist_tiny}{{c}{45}}
\newlabel{fig: token_pp_hist_distilbert}{{A.1d}{45}}
\newlabel{sub@fig: token_pp_hist_distilbert}{{d}{45}}
\newlabel{fig: token_pp_hist_roberta}{{A.1e}{45}}
\newlabel{sub@fig: token_pp_hist_roberta}{{e}{45}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.1}{\ignorespaces The token count distribution for the full dataset of 3,449 tweets for all models.\relax }}{45}{}\protected@file@percent }
\newlabel{fig: apdxa_tokens}{{A.1}{45}}
\@writefile{toc}{\contentsline {section}{\numberline {A.4}Confusion matrix from every test run}{45}{}\protected@file@percent }
\newlabel{sec: conf_matrix_test_runs}{{A.4}{45}}
\@writefile{lot}{\contentsline {table}{\numberline {A.3}{\ignorespaces Confusion matrix from every test run for the experiments set 1.\relax }}{46}{}\protected@file@percent }
\newlabel{tab: conf_matrix_test_runs}{{A.3}{46}}
\@writefile{toc}{\contentsline {section}{\numberline {A.5}Mean token length by domain}{46}{}\protected@file@percent }
\newlabel{sec: avg_token_length_domains}{{A.5}{46}}
\@writefile{lot}{\contentsline {table}{\numberline {A.4}{\ignorespaces Mean token lengths for each domain\relax }}{46}{}\protected@file@percent }
\newlabel{tab: avg_token_length_domains}{{A.4}{46}}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Other references}{47}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {B.1}Additional information on environment used}{47}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {B.2}References for transformer models used in experiment sets 1 and 2}{47}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {B.1}{\ignorespaces The transformer models used for the experiments and links to their documentation.\relax }}{47}{}\protected@file@percent }
\newlabel{tab: apdxb_model_doc}{{B.1}{47}}
\@writefile{toc}{\contentsline {section}{\numberline {B.3}References for models used in data exploration}{47}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {B.2}{\ignorespaces Sentiment analysis model and link to its documentation.\relax }}{47}{}\protected@file@percent }
\newlabel{tab: apdxb_other_model_doc}{{B.2}{47}}
\@writefile{toc}{\contentsline {section}{\numberline {B.4}References for evaluation metrics used in experiment sets 1 and 2}{48}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {B.3}{\ignorespaces The metrics used for evaluating the performance of the models in the experiments and links to their documentation.\relax }}{48}{}\protected@file@percent }
\newlabel{tab: apdxb_metric_doc}{{B.3}{48}}
\gdef \@abspage@last{56}
